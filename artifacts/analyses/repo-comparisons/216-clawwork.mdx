---
title: "Brainstorm: HKUDS/ClawWork Analysis"
description: Analysis of HKUDS/ClawWork for epic #163
type: brainstorm
---

## Context

**GitHub sub-issue:** [#216](https://github.com/roxabi/boilerplate/issues/216)
**Repository:** [HKUDS/ClawWork](https://github.com/HKUDS/ClawWork)
**Stars / Activity:** 985 stars, last commit 2026-02-16 (launched 2026-02-15 â€” 2 days old, very active)

## Summary Table

| Axis | Rating | One-liner |
|------|--------|-----------|
| What it does | ðŸŸ¡ | AI agent economic benchmark â€” different domain, not a SaaS boilerplate |
| How it works | ðŸŸ¢ | Economic pressure loop with token-cost tracking is genuinely novel |
| Architecture | ðŸŸ¡ | Clean Python layering; no monorepo, no TypeScript â€” apples vs oranges |
| File structure | ðŸ”´ | Flat structure, no package boundaries, minimal conventions |
| Tech stack | ðŸ”´ | Python/pip ecosystem vs Bun/TypeScript â€” no portability |
| DX | ðŸŸ¡ | Two-terminal quick start is frictionless; env management comparable |
| Testing | ðŸ”´ | Ad-hoc test scripts scattered in `scripts/`; no test runner or coverage |
| CI/CD | ðŸ”´ | Single GitHub Pages deploy workflow; no lint, type, test, or preview stages |
| Documentation | ðŸŸ¢ | Exceptional README with architecture diagram, examples, and live leaderboard |
| Unique ideas | ðŸŸ¢ | Economic survival benchmark, work/learn agent decision loop, TrackedProvider |

## Detailed Analysis

### 1. What It Does ðŸŸ¡

ClawWork is an **economic benchmark and simulation framework** for AI agents. It is not a SaaS boilerplate â€” the comparison is domain-mismatched by design. Its purpose:

- Transforms AI assistants (Claude Code-style agents) into "AI coworkers" with economic accountability.
- Evaluates agent performance across 220 professional tasks from the [GDPVal dataset](https://openai.com/index/gdpval/), spanning 44 occupations (technology, finance, healthcare, legal).
- Measures three production-relevant metrics: **work quality**, **cost efficiency**, and **economic survival** â€” agents start with $10 and must earn income while paying per token.
- Provides a live public leaderboard at [hkuds.github.io/ClawWork](https://hkuds.github.io/ClawWork/) showing real-time AI earnings.

**Target audience:** AI researchers, LLM teams benchmarking models, developers building production AI agents who want cost-efficiency signals.

**Problem it solves:** Traditional benchmarks (MMLU, HumanEval) measure knowledge but not cost-efficiency or production viability. ClawWork introduces economic pressure as a selection mechanism.

Rating is ðŸŸ¡ because the repo is interesting adjacent inspiration, not a direct comparator.

### 2. How It Works ðŸŸ¢

The core loop is elegant and novel:

1. Agent receives a GDPVal professional task assignment.
2. Agent decides: **Work** (attempt the task for income) or **Learn** (invest in knowledge base to improve future performance). This mirrors real career trade-offs.
3. Agent executes via 8 specialized tools: `decide`, `submit`, `learn`, `status`, `search`, `create`, `execute`, `video`.
4. Work submissions are evaluated by GPT-4o using category-specific rubrics calibrated to each of the 44 GDPVal sectors.
5. Income is earned or deducted; token costs are tracked per-message via `TrackedProvider` (a provider wrapper that intercepts LLM calls).
6. State persists to disk; the React dashboard polls via WebSocket for live updates.

**ClawMode integration** is the standout feature: a `LiveBenchAgentLoop` subclass that wraps any Nanobot gateway, adding economic tracking as a drop-in layer without modifying the host agent's behavior.

The `TrackedProvider` pattern (wrapping an LLM provider to intercept token counts) is directly applicable to any system that needs cost observability on AI calls.

### 3. Architecture & Layers ðŸŸ¡

```
clawmode_integration/     # Drop-in Nanobot adapter (agent_loop.py, provider_wrapper.py, tools.py)
livebench/                # Core benchmark: agent, API, tools, scheduler, data, work
eval/                     # Evaluation prompt generation and testing
scripts/                  # Economic analysis, data generation, E2B template utilities
frontend/                 # React + Vite + Tailwind dashboard (standalone)
```

**Strengths:**
- Clean separation between benchmark core (`livebench/`) and integration adapter (`clawmode_integration/`).
- The `TrackedProvider` / `AgentLoop` subclassing pattern is a solid OOP layering approach.
- FastAPI backend + React frontend with WebSocket for real-time data â€” straightforward and functional.

**Weaknesses vs roxabi:**
- No monorepo tooling â€” frontend and backend share a flat root with no enforced boundaries.
- No dependency injection framework (NestJS equivalent) â€” the Python backend is procedural.
- No hexagonal or DDD patterns; the `livebench/` code mixes orchestration and domain logic.
- No shared type definitions across frontend/backend â€” JSON APIs are untyped at the seam.

### 4. File / Project Structure ðŸ”´

```
ClawWork/
â”œâ”€â”€ .env.example
â”œâ”€â”€ .github/workflows/deploy.yml
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ assets/                       # Images and GIFs for README
â”œâ”€â”€ clawmode_integration/         # Nanobot adapter
â”‚   â”œâ”€â”€ agent_loop.py
â”‚   â”œâ”€â”€ cli.py
â”‚   â”œâ”€â”€ provider_wrapper.py
â”‚   â”œâ”€â”€ skill/
â”‚   â””â”€â”€ tools.py
â”œâ”€â”€ eval/                         # Evaluation meta-prompts
â”œâ”€â”€ frontend/                     # React dashboard
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ src/
â”‚   â””â”€â”€ vite.config.js
â”œâ”€â”€ livebench/                    # Core backend
â”‚   â”œâ”€â”€ agent/, api/, configs/, data/, prompts/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ scheduler/, tools/, trading/, utils/, work/
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ requirements.txt              # Root-level Python deps (livebench only)
â”œâ”€â”€ run_test_agent.sh
â”œâ”€â”€ scripts/                      # Analysis and utility scripts
â”œâ”€â”€ setup.py
â”œâ”€â”€ start_dashboard.sh
â””â”€â”€ view_logs.sh
```

**Key gaps vs roxabi:**
- No package-level `package.json` equivalents â€” frontend is semi-isolated but not in a monorepo.
- `requirements.txt` at root only covers the backend; frontend uses `package-lock.json` (npm, not Bun).
- No `turbo.json` or equivalent pipeline orchestration â€” build steps are manual shell scripts.
- Configuration files (`biome.json`, `commitlint.config.cjs`, `lefthook.yml`) have no equivalents.
- Shell scripts (`start_dashboard.sh`, `run_test_agent.sh`) replace proper npm/bun scripts.

### 5. Tech Stack & Tooling ðŸ”´

| Concern | ClawWork | roxabi_boilerplate |
|---------|----------|-------------------|
| Runtime | Python 3.10+ | Bun 1.3.9 |
| Backend | FastAPI + uvicorn | NestJS + Fastify |
| Frontend | React + Vite + Tailwind (JS) | TanStack Start + Tailwind (TS) |
| Agent framework | LangChain + LangGraph + MCP | Custom (NestJS services) |
| Package manager | pip / conda | Bun workspaces |
| Monorepo | None | TurboRepo |
| Linter/formatter | None detected | Biome |
| Type system | Python type hints (partial) | TypeScript strict |
| Testing | ad-hoc scripts | Vitest + Playwright |
| E2B sandboxing | Yes (code execution) | No |
| Tavily / Jina search | Yes | No |

The Python stack is not portable to roxabi, but the **LangChain + LangGraph + MCP** combination is worth tracking as a backend agent framework reference. The **E2B sandbox** for code execution is also a notable addition roxabi does not have.

### 6. Developer Experience (DX) ðŸŸ¡

**Positives:**
- Ultra-fast first run: `git clone` + `pip install -r requirements.txt` + copy `.env.example` â†’ two shell commands â†’ running.
- The two-terminal pattern (`./start_dashboard.sh` and `./run_test_agent.sh`) is documented with expected console output â€” low cognitive load for first-timers.
- The `.env.example` is exceptionally well-commented: every variable has a description, provider options, and three worked configuration examples (OpenAI-only, SiliconFlow+OpenAI, SiliconFlow-only).
- Live browser feedback at `localhost:3000` immediately rewards setup effort.

**Weaknesses vs roxabi:**
- No hot-reload for backend development â€” uvicorn restarts are manual.
- No editor config, no Biome/ESLint for the frontend JS code.
- No `bun run dev` equivalent; shell scripts are not cross-platform (Windows users need WSL/Git Bash).
- `conda create` as onboarding recommendation adds friction for Python developers who prefer `venv` or `uv`.
- No Docker Compose for local development (roxabi has `docker-compose.yml` for the database).

**Notable DX win to borrow:** The `.env.example` commenting style â€” roxabi's `.env.example` could benefit from the same pattern of grouping variables by concern with inline rationale and multiple worked configuration examples.

### 7. Testing Strategy ðŸ”´

ClawWork has no formal test infrastructure:

- No test runner (`pytest`, `jest`, `vitest`) configured at root.
- `pytest.ini` is absent; test discovery would need to be set up manually.
- Ad-hoc test files scattered across `scripts/`:
  - `test_e2b_template.py`
  - `test_economic_tracker.py`
  - `test_task_exhaustion.py`
  - `test_task_value_integration.py`
  - `validate_economic_system.py`
- `eval/test_single_category.py` tests the evaluation pipeline in isolation.
- No coverage targets, no CI test stage, no mocking strategy visible.

The scripts are integration-style scripts that hit real APIs rather than unit tests with mocked dependencies. For a 2-day-old research repo this is understandable, but it would be unacceptable in a production SaaS boilerplate.

**Roxabi advantage:** Vitest with coverage enforcement, Playwright for E2E, and a dedicated `docs/standards/testing.mdx` that mandates test organization and coverage thresholds.

### 8. CI/CD Pipelines ðŸ”´

ClawWork has exactly one workflow: `.github/workflows/deploy.yml`.

**What it does:**
- Triggers on push to `main` when `frontend/**` files change (or manual dispatch).
- Generates static leaderboard data via Python script (`scripts/generate_static_data.py`).
- Builds the React frontend with `npm ci` + `npm run build`.
- Deploys to GitHub Pages via `actions/deploy-pages`.

**What is absent:**
- No lint stage (no linter configured anyway).
- No type-check stage.
- No test stage.
- No preview deployments.
- No backend deployment automation.
- No branch protection or merge rules.
- No secrets management beyond repository-level GitHub secrets (implicit).
- No staging/production separation.

**Roxabi advantage is substantial:** three workflows (`ci.yml`, `deploy-preview.yml`, `neon-cleanup.yml`), Lefthook git hooks with commitlint, Biome auto-format on every edit, and full TurboRepo affected-file optimization.

### 9. Documentation Quality ðŸŸ¢

ClawWork's README is one of the best in the HKUDS organization's recent repos:

- **Architecture diagram** (both an image and an ASCII fallback in comments) â€” rare for a 2-day-old project.
- **Live demo link** prominently placed above the fold.
- **Animated GIF** of the leaderboard â€” communicates value proposition in seconds.
- **Dual quick-start paths** (Standalone simulation vs. Nanobot integration) with expected console output.
- **Full integration section** with step-by-step nanobot ClawMode setup.
- **Well-structured `.env.example`** with grouped sections and multiple worked examples.
- `scripts/README.md` documents each analysis utility script.

**Weaknesses:**
- No CONTRIBUTING.md or CHANGELOG.
- No architecture decision records (ADRs).
- No API documentation for the FastAPI backend.
- No versioning strategy documented.
- Frontend `src/` has no inline documentation.

**Roxabi advantage:** Comprehensive `docs/` MDX site covering architecture, standards, processes, contributing guide, and changelog. ClawWork trades documentation depth for README polish.

**Borrowable pattern:** README-as-demo with animated GIF + live link + expected console output. Roxabi's README is functional but dry â€” adding a short "what it looks like running" section with expected output would improve onboarding.

### 10. Unique / Novel Ideas ðŸŸ¢

These ideas are genuinely novel and worth studying:

**1. Economic survival as benchmark metric**
Instead of accuracy scores, agents are measured on whether they remain solvent. This introduces emergent behaviors (risk management, task selection, frugality) that accuracy benchmarks miss entirely. The "survival tier" (`thriving`, `stable`, `struggling`, `bankrupt`) is a compelling UX for observability.

**2. Work vs. Learn decision loop**
The agent is not just a task executor â€” it makes a meta-decision each iteration: complete a task now (income) or invest in learning (future performance multiplier). This mirrors real human career dynamics and creates a richer evaluation space than single-shot task benchmarks.

**3. TrackedProvider pattern**
Wrapping an LLM provider to transparently intercept and accumulate token costs is a clean, reusable pattern. For roxabi's AI integrations (or any production app using LLMs), this pattern enables cost observability without modifying agent logic. The wrapper subclasses the existing provider and decorates the `generate` method â€” zero coupling.

**4. Dual-provider setup (Agent model vs. Evaluator model)**
Separating the model used for task execution from the model used for quality evaluation (with different API keys/bases) is a practical production pattern. It allows using cheaper/faster models for work while using the most capable model for evaluation. Roxabi's AI integrations could benefit from this separation.

**5. E2B sandbox for code execution**
Using [E2B](https://e2b.dev/) as a secure sandbox for executing agent-generated code is a mature, production-ready approach. This is absent from roxabi and 2ndBrain.

**6. Category-specific evaluation rubrics**
The 44-occupation-specific LLM evaluation rubrics (in `eval/meta_prompts/`) provide nuanced quality signals impossible with generic scoring. For any AI system where output quality varies by domain, domain-specific rubrics are worth exploring.

### 11. What They Do Better Than roxabi_boilerplate

**1. README quality and demo-ability**
ClawWork's README communicates value in 10 seconds via the animated GIF and live leaderboard link. Roxabi's README is technically correct but does not show the product running. **Actionable:** Add a short demo GIF or screenshot to roxabi's README showing the dashboard/login flow.

**2. `.env.example` documentation**
ClawWork groups environment variables by concern (agent model, evaluator model, productivity tools, service config) with inline rationale and three worked configuration examples. Roxabi's `.env.example` is present but less commented. **Actionable:** Adopt the grouped + worked-example pattern.

**3. Economic observability pattern (TrackedProvider)**
For any roxabi app that integrates AI/LLM calls, the `TrackedProvider` pattern provides cost tracking without coupling. **Actionable:** Evaluate adding a provider wrapper layer in `packages/` for LLM cost observability when AI features are added to roxabi.

**4. Dual-model setup for AI evaluation**
Separating task-execution model from evaluation model is a production pattern roxabi does not yet have (since it does not yet have AI features). Worth capturing as a pattern when AI is added.

### 12. What They Do Better Than 2ndBrain

2ndBrain is a personal productivity system (Python + Telegram bot + Google Workspace + knowledge base). ClawWork outperforms it on:

**1. Production observability**
ClawWork has a live React dashboard with WebSocket real-time updates. 2ndBrain has no dashboard â€” observability is via Telegram messages and log files. **Actionable for 2ndBrain:** A lightweight web dashboard for session/task status would be a significant DX improvement.

**2. Structured economic tracking**
ClawWork tracks costs per message/task explicitly. 2ndBrain has `tiktoken` for token counting but no cost tracking or budget enforcement. **Actionable for 2ndBrain:** Add per-session cost tracking and budget alerts.

**3. Frontend architecture**
ClawWork uses a proper React + Vite + Tailwind frontend. 2ndBrain has no web frontend at all. This is not a weakness per se (Telegram is the interface) but limits observability.

**Cross-reference with roxabi comparison:** The strengths are different â€” ClawWork beats 2ndBrain on observability/frontend; it has comparable README quality (both are strong). The TrackedProvider pattern is relevant to both 2ndBrain and roxabi for any AI feature work.

### 13. Key Takeaways

**Priority 1 â€” Must-have patterns**

1. **TrackedProvider / LLM cost tracking pattern** â€” When roxabi adds AI/LLM features, wrap the provider to intercept token counts and report costs per request. This enables budget enforcement, cost dashboards, and per-user billing. File: `clawmode_integration/provider_wrapper.py`.

2. **Dual-model separation (worker vs. evaluator)** â€” For any AI pipeline with a quality evaluation step, use a separate, more capable model for evaluation with its own API key/base. Document this in `.env.example` with configuration examples.

**Priority 2 â€” Nice-to-have improvements**

3. **README demo-ability** â€” Add a GIF or screenshot to roxabi's `README.md` showing the running app. ClawWork communicates value in seconds; roxabi requires reading to understand the product.

4. **`.env.example` commenting style** â€” Adopt ClawWork's pattern of grouping variables by concern with inline rationale and 2-3 worked configuration examples. Reduces onboarding friction.

5. **Economic/survival benchmarking concept** â€” If roxabi ever adds an AI agent feature, the work/learn decision loop and economic survival tier system are genuinely novel evaluation patterns worth implementing or referencing.

**Priority 3 â€” Monitor**

- **E2B sandbox** â€” If roxabi adds code execution capabilities (e.g., for an AI coding feature), E2B is a production-ready sandbox worth evaluating.
- **LangGraph for agent orchestration** â€” ClawWork uses LangGraph for agent state machines. If roxabi's backend grows an AI agent subsystem, LangGraph is a more structured alternative to ad-hoc async loops.

**Potential GitHub issues to create:**

- `feat: add TrackedProvider pattern to packages/ for LLM cost observability`
- `chore: improve .env.example with grouped sections and worked configuration examples`
- `docs: add demo GIF/screenshot to README showing running app`

## What's next?

The most actionable takeaway is the **TrackedProvider pattern** â€” it is portable, small, and directly applicable whenever roxabi adds LLM integration. The `.env.example` documentation style improvement is trivially low-effort with meaningful onboarding impact.

ClawWork is too domain-specific (benchmark/research) to inspire architectural changes to a SaaS boilerplate, but its **observability patterns** (cost tracking, live dashboard, economic survival tiers) represent a mature approach to AI feature monitoring that roxabi should plan for proactively.
