---
title: "Enhance review + 1b1 with root cause analysis, recommendations & confidence-based auto-apply"
description: Analysis of enriching the review and 1b1 skills with deeper finding analysis and confidence-gated auto-apply
---

**Issue:** #305 (sub-issue of #251)
**Status:** Analysis
**Tier:** F-lite (score 4/10)

## Context

The `/review` skill currently produces Conventional Comment findings (issue, suggestion, nitpick, praise) with file paths and line numbers — but each finding is a surface-level observation. Reviewers say *what* is wrong without explaining *why* it exists or *how* to fix it. The `/1b1` walkthrough then presents these thin findings to the user one at a time, forcing them to investigate root causes themselves.

Meanwhile, many review findings are straightforward fixes (typos, missing error handling, naming inconsistencies) that don't need human judgment. Today, every finding — trivial or complex — goes through the same manual 1b1 flow.

## Questions Explored

1. How should review findings be enriched to provide actionable context (root cause, recommendation)?
2. Where in the pipeline should enrichment happen?
3. How can high-confidence findings be auto-applied without user interaction?
4. How should the 1b1 skill adapt to present richer findings for low-confidence items?

## Analysis

### Current Pipeline

```
/review → Phase 2 (domain agents) → Phase 3 (merge) → Phase 4 (1b1 walkthrough) → fixer
```

Each review agent produces flat Conventional Comments. The 1b1 walkthrough presents each finding with: comment, severity, file path, line number, reviewer attribution. The user decides accept/reject/defer per item. Accepted findings go to fixer agents.

### Enrichment Requirements

Three new fields are added to the existing finding format (Conventional Comment + file path + line number + reviewer attribution):

| Field | Description | Example |
|-------|-------------|---------|
| **Root cause** | Why this issue exists — not just what it is | "The `sql.raw()` call was added in the migration from Knex without parameterization" |
| **Recommendation** | Concrete fix action (primary) | "Replace `sql.raw(input)` with `sql.param(input)` using Drizzle's parameterized API" |
| **Alternative solutions** | 2-3 possible fixes for the 1b1 flow | "1. Parameterize with Drizzle API (recommended). 2. Use prepared statements. 3. Input validation + escaping." |
| **Confidence** | 0-100% score — how confident the reviewer is in both diagnosis and recommendation | 85% (known pattern, clear fix) vs 40% (uncertain root cause, speculative fix) |

### Example: Before and After Enrichment

**Before (current format):**

```
issue(blocking): This `sql.raw()` call with user input is a SQL injection vector.
  apps/api/src/users/users.service.ts:42
  — security-auditor
```

**After (enriched format):**

```
issue(blocking): This `sql.raw()` call with user input is a SQL injection vector.
  apps/api/src/users/users.service.ts:42
  — security-auditor
  Root cause: The raw SQL call was introduced during the Knex-to-Drizzle migration
    without converting to parameterized queries.
  Solutions:
    1. Replace sql.raw(input) with sql.param(input) using Drizzle's parameterized API (recommended)
    2. Use prepared statements with explicit parameter binding
    3. Add input validation + escaping as a defense-in-depth layer
  Confidence: 92%
```

### Confidence Scoring Criteria

Confidence should reflect both **diagnostic certainty** (is the finding correct?) and **fix certainty** (is the recommendation the right fix?):

| Score Range | Meaning | Examples |
|-------------|---------|----------|
| 90-100% | Obvious pattern, deterministic fix | Missing null check, unused import, typo, naming convention |
| 70-89% | High confidence with minor judgment | Missing error handling (clear pattern), refactor opportunity (well-documented approach) |
| 50-69% | Moderate confidence, needs human review | Architectural suggestion, performance concern, design trade-off |
| 0-49% | Speculative or uncertain | Potential race condition, unclear requirements, edge case hypothesis |

**Important:** Confidence is orthogonal to severity. A blocking security finding can have low confidence (uncertain if the vulnerability is exploitable). A non-blocking nitpick can have high confidence (definitely a style issue).

**Calibration risk:** Agent-generated confidence scores are heuristic, not ground truth. Agents may over-estimate confidence on speculative findings. Future calibration could compare predicted confidence against human accept/reject rates, but this is deferred — the 80% threshold provides a conservative starting buffer.

### Pipeline Integration

The auto-apply gate slots into the existing review pipeline between Phase 3 (merge) and Phase 4 (1b1):

**Before:**

```
/review → Phase 2 (domain agents) → Phase 3 (merge) → Phase 4 (1b1) → fixer
```

**After:**

```
/review → Phase 2 (enriched agents) → Phase 3 (merge + sort by confidence)
       → Phase 3.5 (auto-apply ≥80%) → Phase 4 (enriched 1b1 for <80%) → fixer
```

**Auto-apply mechanics (Phase 3.5):** Findings with confidence ≥ 80% are batched and passed to fixer agents (the same fixer agents used in Phase 4 today). The review skill itself does not edit code — it spawns fixer agent(s) scoped to the high-confidence findings. This reuses the existing fixer infrastructure with no new agents.

**Auto-apply scope:** Only actionable finding categories are eligible for auto-apply: `issue`, `suggestion`, `todo`. Non-actionable categories (`praise`, `thought`, `question`) are exempt — praise is reported in the summary, thoughts/questions always go to 1b1.

**Auto-apply failure handling:** If a fixer agent fails to apply a high-confidence fix (test failure, lint error, architectural complexity), the finding is demoted to the 1b1 queue with a note: "Auto-apply attempted but failed: {reason}." The user then decides manually. No silent regressions.

**Post-apply summary:** After auto-apply completes, the user sees a grouped summary before the 1b1 walkthrough begins:

```
── Auto-Applied Fixes (confidence ≥ 80%) ──

Applied 4 findings automatically:
  1. ✅ issue(blocking): SQL injection in users.service.ts:42 (92%)
  2. ✅ suggestion(blocking): Missing null check in auth.guard.ts:18 (88%)
  3. ✅ nitpick: Unused import in dashboard.tsx:3 (95%)
  4. ✅ suggestion(non-blocking): Rename variable per convention in api.ts:27 (85%)

Remaining 3 findings (confidence <80%) will be presented one-by-one.
```

### Enriched 1b1 Brief Format

For findings with confidence &lt;80%, the 1b1 walkthrough presents an enriched brief:

```
── Item 2/3: Missing error boundary in dashboard ──

issue(blocking) — 62% confidence — architect
  apps/web/src/routes/dashboard.tsx:15

Root cause: The dashboard route renders async data without an ErrorBoundary,
  meaning any fetch failure crashes the entire page instead of showing a fallback.

Solutions:
  1. Wrap the dashboard in a React ErrorBoundary with a retry-capable fallback (recommended)
  2. Add try/catch in the route loader and return an error state
  3. Use TanStack Router's built-in errorComponent prop

Recommended: Solution 1 — ErrorBoundary is the React-idiomatic pattern and handles
  both render errors and async failures. The other solutions only cover loader errors.
```

The user then decides: Fix now (with which solution), Reject, Skip, or Defer.

### Safety Rule Update

The current review skill Safety Rule 3 states: "Human decides on every finding — findings go through 1b1 walkthrough before any fix is applied." This invariant is intentionally relaxed for high-confidence findings. The updated rule becomes:

> **Safety Rule 3 (updated):** Human decides on every finding with confidence &lt;80%. Findings with confidence ≥80% are auto-applied by fixer agents, with results shown in a post-apply summary. The human can review auto-applied changes via git diff at any time.

This preserves human oversight for uncertain findings while allowing deterministic fixes to proceed without friction.

### Preserved Invariants (Regression Baseline)

The following behaviors must remain unchanged:

- Blocker findings with confidence &lt;80% always surface to the human in 1b1
- All 1b1 decision options remain: Fix now, Reject, Skip, Defer
- PR comment format (Phase 3.5) is preserved — enriched findings add fields but don't change the grouped structure
- Fixer agent workflow is unchanged — it receives findings and applies fixes
- Verdict logic (blockers → request changes, etc.) is unchanged
- Fresh agent requirement (no implementation context in reviewers) is preserved

## Shapes

### Shape A: Enrich at Review Agent Level (Selected)

- **Description:** Modify the Phase 2 prompt so each domain reviewer (security-auditor, architect, frontend-dev, etc.) outputs enriched findings directly. Each finding includes root cause, recommendation, alternative solutions, and confidence alongside the existing Conventional Comment format.
- **Trade-offs:**
  - Pro: Simplest change — prompt modifications only, no new pipeline stages
  - Pro: Reviewers already have full context (diff, file contents, spec) to generate root causes
  - Pro: Confidence comes from the domain expert who found the issue
  - Con: Output format is larger — each finding grows from ~3 lines to ~10-12 lines
  - Con: Confidence quality depends on each reviewer agent's calibration (mitigated by conservative threshold)
- **Rough scope:** 3 files:
  - `review/SKILL.md`: Phase 2 agent prompt (enriched output format), new Phase 3.5 (auto-apply gate + fixer spawn), Phase 3 merge format update, Safety Rule 3 update
  - `1b1/SKILL.md`: Enriched brief template (root cause, solutions list, recommended solution)
  - `fixer.md`: Accept enriched finding format, use recommendation field to guide fixes rather than re-deriving

### Shape B: Separate Enrichment Pass

- **Description:** Review agents produce findings as today. A new pipeline stage post-Phase 3 enriches each finding with root cause, recommendation, and confidence via a dedicated enrichment agent.
- **Trade-offs:**
  - Pro: Review agents unchanged — lower risk of regression
  - Pro: Dedicated enrichment agent can be tuned independently
  - Con: Extra pipeline stage adds latency and token cost
  - Con: Enrichment agent lacks the reviewer's original reasoning context
  - Con: More complex pipeline to maintain
- **Rough scope:** 4+ files (new enrichment logic, review/SKILL.md pipeline update, 1b1/SKILL.md, fixer.md)

### Shape C: Two-Pass Validation

- **Description:** Review agents produce findings, then a second set of validation agents independently re-evaluates each finding and scores confidence. Agreement between original reviewer and validator increases confidence.
- **Trade-offs:**
  - Pro: Higher quality confidence scores through consensus
  - Pro: Catches false positives from reviewers
  - Con: Doubles agent cost (2x token spend per finding)
  - Con: Significantly more complex pipeline
  - Con: Diminishing returns for most findings
- **Rough scope:** 5+ files + new validation agent definition

## Fit Check

| Requirement | Shape A | Shape B | Shape C |
|-------------|---------|---------|---------|
| Root cause per finding | ✅ | ✅ | ✅ |
| Recommendation per finding | ✅ | ✅ | ✅ |
| Solutions list (2-3 alternatives) | ✅ | ✅ | ✅ |
| Confidence scoring | ✅ | ✅ | ✅ |
| Minimal pipeline changes | ✅ | ❌ | ❌ |
| No new agents required | ✅ | ❌ | ❌ |
| Reviewer context preserved | ✅ | ❌ | ✅ |
| Low token cost overhead | ✅ | ❌ | ❌ |
| Existing functionality preserved | ✅ | ✅ | ✅ |

**Selected shape:** Shape A — Enrich at review agent level. Simplest approach, preserves reviewer context, no new pipeline stages. Auto-apply reuses existing fixer agents.

## Conclusions

1. **Enrichment at source is optimal.** Review agents already have the full context needed to explain root causes, recommend fixes, and provide alternatives. Adding enrichment as a prompt requirement is the simplest, cheapest change.

2. **Confidence-gated auto-apply is safe with guardrails.** The 80% threshold sits at the lower bound of the "high confidence" band (70-89%), requiring both high diagnostic certainty and high fix certainty. Auto-applied fixes are visible in a post-apply summary. Failed auto-applies demote to 1b1. If quality degrades, the threshold can be raised.

3. **1b1 enrichment is additive.** The existing 1b1 flow (brief → decide → execute) gains richer context per item. Each item now shows: point, root cause, 2-3 solutions, and recommended solution with rationale. The existing options (Fix now, Reject, Skip, Defer) remain unchanged.

4. **Safety Rule 3 requires explicit update.** The current "human decides on every finding" invariant is intentionally relaxed for high-confidence findings. The updated rule preserves human oversight for uncertain findings while allowing deterministic fixes to proceed.

5. **Minimal blast radius.** All changes are prompt/skill modifications across 3 files. No application code, no new agents, no pipeline restructuring. The fixer agent receives enriched findings but its workflow is unchanged.

## Open Questions

- **Confidence calibration over time:** Should we track predicted confidence vs actual human accept/reject rates to improve scoring? Deferred — start with the heuristic approach and evaluate after a few review cycles.
- **User anchoring on confidence numbers:** Users may treat agent confidence as ground truth. The 1b1 brief should frame confidence as a heuristic ("estimated confidence") rather than a certainty score.
- **PR comment format for enriched findings:** Should the Phase 3.5 PR comment include the full enriched format (root cause, solutions) or stay compact? Defer to spec — likely compact for PR comments, full for 1b1.

## Next Steps

- Create spec at `artifacts/specs/305-review-1b1-enhancements.mdx` with detailed implementation plan
- Implementation touches:
  - `review/SKILL.md`: Phase 2 agent prompt + new Phase 3.5 + Safety Rule 3 update
  - `1b1/SKILL.md`: Enriched brief template with solutions list
  - `fixer.md`: Accept enriched finding format, use recommendation to guide fixes
