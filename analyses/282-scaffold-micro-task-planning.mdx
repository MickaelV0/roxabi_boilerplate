---
title: "Scaffold Micro-Task Planning"
description: Analysis for improving scaffold planning with spec-driven micro-tasks, verification commands, committed plan artifacts, and cross-artifact consistency checks.
---

**Status:** Draft — Feb 2026
**Issue:** #282 (Phase 3 of #251 AI Workflow Improvement Strategy)
**Tier:** F-full (score 4/10, upgraded due to execution model change)
**Dependencies:** #280 (complexity scoring — closed), #281 (bootstrap enrichment — closed)

## Context

The Roxabi scaffold skill (`/scaffold`) currently plans implementation via a text-based task list (Step 2d) that agents receive as part of their prompt. This approach has three problems:

1. **Task granularity is inconsistent** — Tasks range from "implement entire API module" to "add a field to a type", with no standard sizing. Agents receive vague instructions and must independently determine their execution plan.
2. **No verification feedback loop** — Agents implement tasks and hope they work. Verification only happens at the quality gate (`bun lint && bun typecheck && bun test`) after all tasks are complete. Failures are expensive to debug because the root cause is unclear.
3. **Plans are ephemeral** — The implementation plan exists only in the conversation context. If a scaffold session is interrupted, the plan is lost. There's no way to resume, review, or learn from previous plans.

Phase 1 (#280) delivered a complexity scoring rubric (1-10 scale → S/F-lite/F-full tiers). Phase 2 (#281) enriched bootstrap with Breadboard tables (UI→Code→Data affordances) and Slices (demo-able vertical increments). These provide the structured inputs that make spec-driven micro-task generation possible.

**Current scaffold flow (relevant steps):**
```
Step 2d: Break into tasks (text-based, agent-assigned)
Step 2e: Present plan for approval
Step 4e: Verify and commit scaffold stubs
Step 5:  Implement (agents receive plan text + spec)
```

**Target scaffold flow:**
```
Step 2d: Break into tasks (text-based, agent-assigned) — unchanged
Step 2e: Present plan for approval — unchanged
Step 4e: Verify and commit scaffold stubs — unchanged
Step 4f: Generate micro-tasks (NEW) — SKIPPED for Tier S
         → Parse spec Breadboard + Slices
         → Expand into micro-tasks with verification commands
         → Run consistency check
         → Commit plan to plans/{issue}-{slug}.mdx
         → Create TaskCreate entries
         → User approves micro-task queue
Step 5:  Implement (agents consume micro-tasks from TaskCreate) — MODIFIED for F-lite/F-full
```

**Tier S bypass:** Step 4f is skipped entirely for Tier S features. Tier S uses direct single-session implementation (existing Step 5 path) — micro-tasks are too granular for 1-3 file changes. This aligns with the parent spec (#251): "Scaffold micro-tasks are too granular for simple features — Tier S bypasses micro-task step entirely."

## Questions Explored

1. How should micro-tasks be generated from spec content?
2. What should a micro-task contain (granularity, verification, metadata)?
3. How should task count scale with complexity?
4. How should parallelization safety be determined?
5. What does the committed plan artifact look like?
6. How strict should the cross-artifact consistency check be?
7. How should agents consume micro-tasks vs the current text plan?

## Analysis

### Micro-Task Anatomy

Each micro-task is a 2-5 minute unit of work with everything an agent needs to execute without ambiguity:

| Field | Description | Example |
|-------|-------------|---------|
| **Description** | What to implement and why | "Add email validation to auth service" |
| **File path** | Exact file(s) to create/modify | `apps/api/src/auth/validator.ts:L25` |
| **Code snippet** | Lightweight skeleton (function signatures, not full logic) | `export function validateEmail(email: string): boolean { ... }` |
| **Verification command** | Executable check that confirms completion | `bun run test apps/api/test/auth.test.ts` |
| **Expected output** | What success looks like | `1 passing test: "validate email format"` |
| **Time estimate** | 2-5 minutes per task | `3 min` |
| **[P] marker** | Parallel-safe flag | `[P]` if no file/import conflicts |
| **Agent assignment** | Which agent owns this task | `backend-dev` |
| **Spec trace** | Which criterion/affordance this covers (unified format: `SC-N` for criteria, `U/N/S{id}` for affordances) | `SC-3`, `N1→S1` |

### Spec-Driven Generation Algorithm

The generation algorithm has two modes based on spec content:

**Primary mode (post-#281 specs with Breadboard + Slices):**

1. Parse `## Breadboard` sections: extract UI affordances (U*), Code affordances (N*), Data stores (S*)
2. Parse `## Slices` section: extract vertical increments (V1, V2, ...)
3. For each slice, in order:
   a. Identify the affordances it covers (from the Slices table)
   b. For each affordance, generate 1-3 micro-tasks:
      - Data store tasks: schema, migration, seed (if S*)
      - Code handler tasks: service logic, endpoint wiring (if N*)
      - UI tasks: component, integration, styling (if U*)
   c. Generate test tasks: one per code/UI affordance
   d. Assign agents based on file path (same rules as Step 2c)
   e. Mark [P] for tasks with no file-path or import overlap within the slice

**Fallback mode (pre-#281 specs without Breadboard/Slices):**

1. Parse `## Success Criteria` section: extract criteria as `SC-1`, `SC-2`, etc.
2. For each criterion:
   a. Analyze the criterion text to determine affected files
   b. Generate 1-5 micro-tasks depending on criterion complexity
   c. Generate verification commands using file-type heuristics:
      - `.ts/.tsx` files → prefer `bun run test {adjacent-test-file}` if test exists, otherwise `bun run typecheck`
      - `.md/.mdx` files → `grep -q "{expected content}" {file}`
      - `.json/.yaml` config → `bun run lint` + `grep -q "{key}" {file}`
      - Other → `[manual]` checklist
   d. Assign agents and mark [P] based on file-path analysis

### Verification Command Strategy

Verification commands are executable checks that confirm a micro-task is complete:

| Change type | Verification approach | Example |
|------------|----------------------|---------|
| **Application code** (`.ts/.tsx`) | Unit/integration test | `bun run test apps/api/test/auth.test.ts` |
| **Type definitions** | Typecheck | `bun run typecheck --filter=@repo/types` |
| **Config files** | Lint + specific check | `bun run lint && grep -q "key" path/to/config.json` |
| **Skill/agent files** (`.md`) | Grep for expected content | `grep -q "Step 4f" .claude/skills/scaffold/SKILL.md` |
| **Documentation** (`.mdx`) | File exists + content check | `test -f docs/path.mdx && grep -q "## Section" docs/path.mdx` |
| **Migration files** | Migrate + check schema | `bun run db:migrate && bun run db:generate --check` |

When no automated verification is possible, the task includes `[manual]` and a human-readable checklist instead of a command.

### Dynamic Task Count

Task count scales with complexity score (from #280 rubric), capped at 30 maximum:

| Complexity score | Tier | Task range | Rationale |
|-----------------|------|------------|-----------|
| 1-3 | S | — (bypass) | Tier S skips Step 4f entirely |
| 4-6 | F-lite | 5-15 | Multi-file features, 2-3 agents |
| 7-10 | F-full | 15-30 | Cross-domain features, 4+ agents |

**Cap rationale:** Features exceeding 30 tasks are better split via `/bootstrap` Gate 2.5 (#285) into sub-issues, each scaffolded independently. Forcing 60+ tasks into one scaffold session creates coordination overhead that outweighs micro-task benefits. **Note:** If #285 has not yet landed when a spec hits the 30-task cap, the scaffold should warn the user and suggest manual decomposition into sub-issues rather than silently truncating.

**Task floor:** Every F-lite/F-full scaffold generates at minimum 3 tasks: one setup/infrastructure task, one core implementation task, and one test/verification task. This floor ensures a meaningful task list even for simple features and prevents degenerate single-task expansions.

**Expansion formula:** Each acceptance criterion or breadboard affordance expands into 1-5 micro-tasks based on:
- File count (1 file = 1 task, multi-file = split)
- Logic complexity (simple CRUD = 1 task, complex business logic = 2-3 tasks)
- Test requirement (each testable behavior = 1 test task)

**Large affordance handling:** If a single affordance expands beyond 5 minutes (e.g., a complex migration or multi-step business logic), split it into 2-3 sub-tasks. The 2-5 minute target is a guide, not a hard gate — some tasks may reach 8-10 minutes for inherently atomic operations.

### Parallelization Detection

Two tasks are marked `[P]` (parallel-safe) if they pass both checks:

1. **File-path check:** No common files between the two tasks
2. **Import analysis:** Task A's files do not import modules modified by Task B (and vice versa)

The import analysis is a **design-time estimation**, not runtime analysis:
- For existing files: read their current `import` statements to build a 1-hop dependency set
- For new files (stubs from Step 4): infer planned imports from the spec's file relationship data (Breadboard wiring: N1→S1 implies the handler imports the store)
- Two tasks conflict if their dependency sets overlap

**Limitation:** New-file import analysis is heuristic. Stubs have minimal imports, so the actual import graph is unknowable until implementation. For new-file tasks where planned imports cannot be inferred, default to non-parallel-safe (conservative). The file-path check alone catches the most common conflicts; import analysis adds value primarily for existing-file modifications.

**Special cases:**
- Shared barrel/index files (e.g., `index.ts` re-exports) → force sequential if both tasks add exports
- Test files → always parallel-safe with implementation tasks (tests import but don't modify source)
- Type-only dependencies → parallel-safe (types are additive, not mutative)

### Committed Plan Artifact

The plan is committed to `plans/{issue}-{slug}.mdx` (project root, not inside `docs/`) before agent spawning. This serves three purposes:

1. **Session resumption:** If a scaffold is interrupted, the plan persists on the worktree branch
2. **Review traceability:** PR reviewers can see what was planned vs what was implemented
3. **Retro analysis:** Future `/retro` enhancements can compare plans to outcomes (not yet implemented — forward reference)

**Lifecycle:** Plans are committed to the worktree branch (`feat/{issue}-{slug}`) and included in the PR. After merge, plan files accumulate on main as a historical archive. This is intentional — plan artifacts are lightweight (markdown) and provide long-term traceability. If accumulation becomes a concern, a future cleanup policy can archive them.

**Directory convention:** The `plans/` directory is new and does not exist in the codebase. It lives at project root alongside `analyses/` and `specs/`. It is NOT inside `docs/` (not rendered by Fumadocs). Scaffold creates the directory on first use.

**Plan structure:**

```markdown
---
title: "Plan: {Feature Title}"
issue: {issue_number}
spec: specs/{issue}-{slug}.mdx
complexity: {score}/10
tier: {S|F-lite|F-full}
generated: {ISO timestamp}
---

## Summary

{1-2 sentence overview}

## Agents

| Agent | Task count | Files |
|-------|-----------|-------|
| {agent} | {N} | {file list} |

## Consistency Report

- Criteria covered: {N}/{total}
- Uncovered criteria: {list or "none"}
- Tasks without spec backing: {list or "none"}

## Micro-Tasks

### Slice V1: {Description}

#### Task 1: {Description} [P] → {agent}
- **File:** {path}
- **Snippet:** {code}
- **Verify:** `{command}`
- **Expected:** {output}
- **Time:** {N} min
- **Traces:** {SC-N, U1→N1→S1}

#### Task 2: {Description} → {agent}
...
```

### Cross-Artifact Consistency Check

After micro-task generation, the scaffold runs a bidirectional consistency check:

**Spec → Tasks (coverage check):**
- Every success criterion must have at least one micro-task with a matching `Traces` field
- Every breadboard affordance must appear in at least one task
- Missing coverage → warning in the consistency report

**Tasks → Spec (gold plating check):**
- Every micro-task must trace back to a spec criterion or breadboard affordance
- Tasks without spec backing → flagged as "gold plating" with required justification
- Acceptable gold plating (exempt categories):
  - **Infrastructure:** DB migrations, schema changes, config file updates
  - **Quality:** Linting fixes, formatting, barrel/index export updates
  - **Build:** Package.json changes, build configuration, environment setup
  - **Documentation:** Inline code comments, JSDoc, README updates required by the feature

Tasks in exempt categories are not flagged. All other tasks without spec backing require justification.

**Gap handling:** The consistency report is presented to the user via the `AskUserQuestion` tool before agent spawning. Options: approve as-is, add missing tasks (scaffold auto-suggests tasks for uncovered criteria), remove flagged tasks, or return to spec.

### Modified Step 5: Agent Micro-Task Consumption

Step 5 changes from text-based plan delivery to micro-task queue consumption:

**Current flow:**
```
Spawn agent → agent receives: spec text + plan text + file stubs
Agent independently decides execution order
```

**New flow (preserving RED→GREEN→REFACTOR):**
```
Phase 1 (RED):   Tester receives test micro-tasks → writes failing tests per slice
Phase 2 (GREEN): Domain agents receive implementation micro-tasks → implement to pass tests
                  Verification runs after each task (test commands now have real test bodies)
Phase 3 (REFACTOR): Domain agents refactor while keeping tests green
```

Verification commands on implementation tasks are marked `[deferred]` until the tester completes RED for their slice. Domain agents run verification only after the corresponding test task is complete. This prevents false failures on TODO stub tests.

**Ordering protocol:** Within each slice, the tester completes all RED tasks before domain agents start GREEN tasks for that slice. Across slices, this can be pipelined: tester works on V2's RED tasks while domain agents work on V1's GREEN tasks.

**TaskCreate metadata per task:**
```json
{
  "taskDifficulty": 3,
  "verificationCommand": "bun run test apps/api/test/auth.test.ts",
  "verificationStatus": "deferred",
  "expectedOutput": "1 passing test",
  "estimatedMinutes": 3,
  "parallel": true,
  "specTrace": "SC-3, N1→S1",
  "slice": "V1",
  "phase": "GREEN"
}
```

`taskDifficulty` (1-5) is a per-task estimate distinct from the feature-level complexity score (1-10) in #280. 1 = trivial single-line change, 5 = complex multi-concern logic.

`verificationStatus`: `"ready"` for tasks with immediately runnable verification (grep checks, typecheck), `"deferred"` for tasks whose verification depends on test bodies being written first (RED phase must complete).

**Verification failure loop:**
1. Agent checks `verificationStatus` — skip if `"deferred"` and RED phase incomplete for this slice
2. Agent runs verification command
3. If pass → mark task complete, move to next
4. If fail → attempt fix based on error output
5. Re-run verification (retry 1/3)
6. After 3 failures → escalate to lead with: task ID, error output, attempted fixes, affected files

## Shapes

### Shape 1: Spec-Driven Generation (Selected)

- **Description:** Parse spec's Breadboard + Slices sections to generate micro-tasks. Each affordance becomes 1-3 tasks, organized by slice. Falls back to acceptance criteria for pre-#281 specs.
- **Trade-offs:**
  - Pro: Tight coupling to spec ensures consistency. Deterministic output. Enables cross-artifact checking.
  - Pro: Leverages #281 investment (breadboard/slices become actionable, not just documentation).
  - Con: Depends on well-structured specs. Garbage-in-garbage-out if spec is poorly written.
  - Con: Fallback mode (criteria-only) produces less structured tasks.
- **Rough scope:** 1-2 files (SKILL.md primary, reference files for examples). Step 4f is ~100-150 lines of skill instructions.

### Shape 2: LLM-Planned Generation

- **Description:** Feed the full spec + relevant codebase context to the LLM and let it freely plan micro-tasks. Similar to current Step 2d but with the micro-task format enforced.
- **Trade-offs:**
  - Pro: Flexible — handles any spec format without parsing rules.
  - Pro: Can discover implicit tasks (infrastructure, config) that spec doesn't explicitly mention.
  - Con: Non-deterministic — same spec may produce different task lists on re-runs.
  - Con: Harder to validate consistency (no structured mapping from spec to tasks).
  - Con: Higher token cost (full spec + codebase in context).
- **Rough scope:** 1 file (SKILL.md). Step 4f is ~50-80 lines (mostly a prompt template).

### Shape 3: Template-Based Generation

- **Description:** Define micro-task templates per file type (NestJS controller = 3 tasks, React component = 4 tasks, etc.). Combine templates based on the spec's file list.
- **Trade-offs:**
  - Pro: Highly deterministic and fast. Templates codify best practices.
  - Pro: Easy to test — each template can be validated independently.
  - Con: Rigid — can't handle spec-specific nuances or novel architectures.
  - Con: Requires maintaining a growing template library.
  - Con: Doesn't use Breadboard/Slices at all — wastes #281 investment.
- **Rough scope:** 1 SKILL.md + 5-10 template files. Higher maintenance burden.

## Fit Check

| Requirement | Shape 1 (Spec-Driven) | Shape 2 (LLM-Planned) | Shape 3 (Template-Based) |
|-------------|----------------------|----------------------|-------------------------|
| Uses Breadboard/Slices from #281 | ✅ | ❌ | ❌ |
| Uses complexity scoring from #280 | ✅ | ✅ | ✅ |
| Deterministic output | ✅ | ❌ | ✅ |
| Cross-artifact consistency checkable | ✅ | ❌ | ❌ |
| Handles pre-#281 specs (fallback) | ✅ | ✅ | ✅ |
| Low maintenance burden | ✅ | ✅ | ❌ |
| Handles novel architectures | ❌ | ✅ | ❌ |

**Selected shape:** Spec-Driven Generation — leverages #281 investment, enables consistency checking, and produces deterministic output. The "novel architectures" gap is mitigated by the fallback mode (criteria-based generation handles any spec format).

### Architect Context Caching

Per #280's recommendation ("Gaps Not Covered by Current Phases"): the committed plan artifact should include bootstrap architect findings so the scaffold architect receives them as context rather than re-reading 30-50 files.

**Implementation:** When the plan artifact is generated at Step 4f, if an analysis document exists at `analyses/{issue}-*.mdx`, include its `## Conclusions` and `## Shapes` (selected shape) sections in the plan under a `## Bootstrap Context` heading. This gives domain agents the architectural rationale without requiring them to discover and read the analysis independently.

### Integration with #283 (Shared Task Lists)

Step 4f creates tasks via `TaskCreate`, which works independently of #283 (shared task lists). If #283 ships after #282:
- **Before #283:** TaskCreate entries are consumed by agents spawned via `Task` (subagents) or `TeamCreate`. Each agent receives its tasks in the spawn prompt. No shared persistent task list.
- **After #283:** Tasks use `CLAUDE_CODE_TASK_LIST_ID={issue}-{slug}` for shared persistent access. Agents claim tasks from the list dynamically.

Both modes are compatible — #282 generates tasks in the same format regardless of whether #283's shared infrastructure exists.

## Conclusions

1. **Micro-tasks are the right granularity for scaffold agents.** 2-5 minute tasks with verification commands replace vague text plans. Each task has everything an agent needs: file path, code snippet, verification command, and expected output.

2. **Spec-driven generation connects the full pipeline.** Bootstrap produces Breadboard + Slices (#281) → scaffold parses them into micro-tasks (#282) → agents execute and verify. The investment in structured specs pays off in structured execution.

3. **The committed plan artifact solves session fragility.** Plans committed to `plans/{issue}-{slug}.mdx` enable session resumption, review traceability, and retro analysis. This addresses a real pain point with the current ephemeral approach.

4. **Cross-artifact consistency checking catches drift early.** Bidirectional spec↔task mapping ensures nothing is missed and nothing is added without justification. This is a lightweight form of requirements traceability.

5. **Task count cap at 30 prevents micro-task abuse.** Features needing 30+ tasks should be split via #285 (Gate 2.5), not crammed into one scaffold session.

6. **Import-aware parallelization is worth the complexity.** File-path-only checks would miss transitive dependencies. The 1-hop import analysis adds modest overhead but prevents subtle race conditions between parallel agents.

## Next Steps

- [ ] Promote this analysis to a spec: `/interview --promote analyses/282-scaffold-micro-task-planning.mdx`
- [ ] Spec must address:
  - Exact SKILL.md Step 4f instructions and modified Step 5 instructions
  - AGENTS.md updates for micro-task consumption rules
  - Pin the Breadboard/Slices MDX format from #281's implementation (reference bootstrap skill template)
  - Plan artifact template and commit sequence (git safety rules)
  - TaskCreate metadata schema (with `taskDifficulty` and `verificationStatus` fields)
  - Tier S bypass guard in Step 4f
- [ ] After spec approval: `/scaffold --spec 282` to implement
- [ ] Validate on 1 real feature scaffold
