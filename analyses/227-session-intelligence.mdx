---
title: "Session Intelligence — Continuous Improvement Cycle with /retro Skill"
description: Analysis for GitHub issue #227 — a session intelligence system that parses Claude Code transcripts, extracts findings, stores them with vector search, and provides trend analysis for continuous improvement
---

## Context

Roxabi Boilerplate has accumulated 758 Claude Code session transcripts as JSONL files in `~/.claude/projects/-home-mickael-projects-roxabi-boilerplate/`. These transcripts contain a wealth of actionable intelligence — recurring blockers, praise for patterns that work well, suggestions from Claude, and nitpicks about code style or process. Today, none of this is systematically captured or analyzed.

This feature builds a **session intelligence system** that:

1. Mechanically parses all session transcripts into a local SQLite database
2. Uses AI-powered classification (via the Claude Code CLI) to extract and categorize findings
3. Provides trend analysis and semantic search for continuous improvement
4. Exposes everything through a `/retro` skill — manually triggered, no automation

**Non-goals (all confirmed out of scope):**

- Cross-project analysis (only `roxabi_boilerplate` sessions)
- Real-time monitoring (no SessionEnd hooks or cron jobs)
- Auto-applying fixes (suggest only, never auto-modify `CLAUDE.md` or standards)
- Web dashboard (CLI and skill interface only)

> **Finding taxonomy:** The four types -- praise, blocker, suggestion, nitpick -- are adapted from Conventional Comments (used in the project's code review standard). They map to developer actions: praise reinforces good patterns, blockers identify problems to fix, suggestions propose improvements, and nitpicks flag minor issues. This alignment with the existing review vocabulary makes findings immediately actionable.

**Prior art:** The 2ndBrain project has a mature knowledge skill using SQLite + FTS5 + sqlite-vec for hybrid search (0.7 vector / 0.3 BM25), `all-MiniLM-L6-v2` embeddings (384 dimensions), and a Python-based CLI. This feature adapts that architecture, rewritten in TypeScript/Bun.

## Questions Explored

1. What AI engine should power finding classification, and how should it be invoked?
2. How should session transcripts be parsed and what metadata is extractable?
3. What storage backend supports both full-text and semantic search without external services?
4. Is `sqlite-vec` viable on Bun/WSL2/Linux, given its alpha status?
5. What embedding model works locally without API keys?
6. How should privacy be handled — what redaction is needed before storing findings?
7. What is the right processing model — batch, parallel, or sequential?
8. How should the pipeline handle idempotency and incremental processing?

## Analysis

### 1. AI Classification Engine — Claude Code CLI

The system shells out to the `claude` CLI for AI-powered classification of session transcripts. This avoids requiring any additional API keys — it uses the developer's existing Claude Code authentication.

**Why not direct API calls:** Direct Anthropic API access would require managing API keys, billing, and rate limits separately from the developer's existing Claude Code subscription. The CLI approach is zero-configuration.

**Processing model:** Sequential — one session at a time. Each session's extracted data is sent to Claude for classification into four finding types: praise, blocker, suggestion, nitpick. Sequential processing is simple, predictable, and avoids overwhelming the CLI with concurrent invocations.

**Trade-off:** Sequential processing of 758 sessions will take significant time on the initial run. However, subsequent runs are incremental (only new sessions), making this a one-time cost that is acceptable for a developer tool.

### 2. Data Sources and Parsing

Three data sources provide session intelligence:

| Source | Location | Content |
|--------|----------|---------|
| Session transcripts | `~/.claude/projects/-home-mickael-projects-roxabi-boilerplate/*.jsonl` | 758 session files with full conversation history |
| Session index | `sessions-index.json` in the same directory | Metadata: summaries, dates, message counts |
| Global history | `~/.claude/history.jsonl` | 7,209 lines of cross-project history (filtered to roxabi only) |

**Phase 1 (mechanical parse)** extracts from JSONL transcripts without any AI cost:

- Session metadata (timestamps, duration, branch, first prompt)
- Tool call success/failure rates
- Error messages and stack traces
- User corrections (patterns where the user redirected Claude)
- Files modified and commands executed
- Message counts and conversation flow

### 3. Storage Architecture — SQLite + sqlite-vec

The storage layer combines three SQLite capabilities:

| Component | Technology | Purpose |
|-----------|-----------|---------|
| Structured data | `bun:sqlite` (built-in) | Sessions, findings, processing log |
| Full-text search | FTS5 (SQLite extension) | BM25-ranked keyword search across findings |
| Vector search | `sqlite-vec` (v0.1.7-alpha.2) | Semantic similarity search via embeddings |

**Database schema:**

```sql
CREATE TABLE sessions (
  id TEXT PRIMARY KEY,
  project_path TEXT,
  git_branch TEXT,
  first_prompt TEXT,
  summary TEXT,
  message_count INTEGER,
  created_at DATETIME,
  modified_at DATETIME,
  duration_minutes REAL,
  processed_at DATETIME,
  analyzed_at DATETIME
);

CREATE TABLE findings (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  session_id TEXT REFERENCES sessions(id),
  type TEXT CHECK(type IN ('praise', 'blocker', 'suggestion', 'nitpick')),
  content TEXT NOT NULL,
  context TEXT,
  severity TEXT CHECK(severity IN ('low', 'medium', 'high')),
  tags TEXT, -- JSON array of strings, e.g. '["auth", "hooks", "performance"]'
  session_timestamp DATETIME
);

CREATE VIRTUAL TABLE findings_fts USING fts5(
  content, context, type,
  content=findings
);

CREATE VIRTUAL TABLE finding_embeddings USING vec0(
  finding_id INTEGER,
  embedding FLOAT[384]
);

CREATE TABLE processing_log (
  session_id TEXT PRIMARY KEY,
  phase TEXT CHECK(phase IN ('parse', 'analyze')),
  processed_at DATETIME,
  status TEXT CHECK(status IN ('success', 'error', 'skipped')),
  error_message TEXT
);
```

> **FTS5 sync requirement:** The `content=findings` directive creates a shadow table. FTS5 content tables do not automatically sync on INSERT/UPDATE/DELETE -- the `schema.ts` module must include triggers or explicit FTS maintenance logic to keep the index consistent with the source table.

**Database location:** `.claude/skills/retro/data/retro.db` (gitignored). This keeps the database alongside the skill definition, separate from application code.

### 4. sqlite-vec Viability on Bun/WSL2

Research confirmed that `sqlite-vec` works with `bun:sqlite` on Linux/WSL2 with no platform blockers.

**Key findings:**

- **Integration pattern:** `import * as sqliteVec from "sqlite-vec"; sqliteVec.load(db);` — straightforward extension loading
- **Alpha status:** v0.1.7-alpha.2 means API breakage is possible. Acceptable risk for developer tooling that is not user-facing.
- **Search algorithm:** Brute-force only (no ANN index). Practical scale limit is approximately 100k-500k vectors. Our use case (&lt;10k vectors for findings) is well within this range.
- **No platform-specific builds needed:** The npm package bundles the correct native binary for the platform.

### 5. Embedding Model — Local Transformers.js

Embeddings are generated locally using `@huggingface/transformers` with the `all-MiniLM-L6-v2` model:

| Property | Value |
|----------|-------|
| Model | `all-MiniLM-L6-v2` |
| Dimensions | 384 |
| Runtime | `@huggingface/transformers` (Transformers.js) |
| Model cache | `~/.cache/huggingface/` (~45 MB) |
| API key required | No |
| Latency | Sub-second per embedding on modern hardware |
| First-run download | ~45 MB (cached after first use) |

The first `/retro` invocation will trigger a one-time model download (~45 MB). To avoid surprising the developer on first use, the spec should include a `retro:setup` command or pre-flight check that downloads the model explicitly with progress feedback.

**Why local embeddings:** No external API dependency, no API key management, no per-query cost, works offline. The 384-dimension output is compact enough for efficient storage and search at our scale.

### 6. Privacy — Regex-Based Secret Redaction

Before storing any finding in the database, a redaction pass removes common secret patterns:

- API keys (patterns like `sk-`, `pk_`, `key_`, etc.)
- Bearer tokens and JWTs
- Database connection strings with credentials
- Environment variable values matching password/secret patterns
- Base64-encoded credentials

The redactor replaces matched patterns with `[REDACTED]` while preserving the surrounding context for the finding to remain useful. This is a best-effort defense — not a guarantee — but appropriate for a local-only developer tool where the transcripts themselves are already on disk unredacted.

### 7. Three-Phase Pipeline

The pipeline is split into three phases with distinct cost profiles:

#### Phase 1: Mechanical Parse (`parse-sessions.ts`)

- Reads all `.jsonl` files from the project-specific Claude directory
- Cross-references with `sessions-index.json` for metadata enrichment
- Extracts structured data: metadata, tool calls, errors, user corrections, files modified
- Stores raw data in the `sessions` table
- **Cost:** Zero (no AI calls). **Idempotent:** Skips sessions already in the database.

#### Phase 2: AI-Powered Analysis (`analyze-findings.ts`)

- Processes sessions sequentially via `claude` CLI
- For each session where `analyzed_at IS NULL`, sends extracted data for classification
- Each finding receives: type (praise/blocker/suggestion/nitpick), content, context, severity (low/medium/high), auto-generated tags
- Generates embeddings for each finding using Transformers.js
- Applies secret redaction before storage
- **Cost:** One Claude CLI invocation per unanalyzed session. **Idempotent:** Only processes sessions not yet analyzed.

**Estimated initial run time:** At 5-15 seconds per session via CLI, the initial run of 758 sessions will take approximately 1-3 hours. This is a one-time cost; subsequent runs process only new sessions.

**Error recovery:** Sessions where the CLI call fails are logged with `status: 'error'` in `processing_log`. Re-running the script resumes from the last unanalyzed session. Partial failures do not require re-processing successful sessions.

**Progress reporting:** The script should display progress (e.g., "Processing session 42/758...") so the developer can monitor the initial run.

#### Phase 3: Recap and Trends (`recap.ts`)

- Queries findings across configurable time periods
- Generates trend analysis: recurring blockers, improvement tracking, regression detection, top suggestions, process evolution
- **Output:** Markdown report suitable for terminal display, plus optional JSON for programmatic consumption
- **Cost:** Zero (database queries only, no AI calls).

### 8. Hybrid Search

Search combines BM25 (keyword) and vector (semantic) results using weighted fusion:

| Component | Weight | Source | Strength |
|-----------|--------|--------|----------|
| Vector search | 0.7 | `sqlite-vec` via `finding_embeddings` | Semantic similarity — finds conceptually related findings even with different wording |
| BM25 search | 0.3 | FTS5 via `findings_fts` | Exact keyword matching — precise when the user knows the right terms |

Weights are tunable. The 0.7/0.3 split is adapted from the 2ndBrain implementation as a starting point. These weights should be tuned empirically once actual finding data is available, since session findings may have different retrieval characteristics than general knowledge notes.

**Available filters:** type, severity, date range, session, tags. Filters are applied before search to reduce the candidate set.

### 9. File Structure

```
.claude/skills/retro/
├── SKILL.md                    # /retro skill definition
├── scripts/
│   ├── parse-sessions.ts       # Phase 1: Parse JSONL transcripts into SQLite
│   ├── analyze-findings.ts     # Phase 2: AI-powered classification (sequential)
│   ├── recap.ts                # Phase 3: Trend analysis and reporting
│   └── search.ts               # Semantic search CLI
├── lib/
│   ├── db.ts                   # SQLite + sqlite-vec setup and connection
│   ├── schema.ts               # DDL: sessions, findings, embeddings tables
│   ├── parser.ts               # JSONL transcript parser
│   ├── embedder.ts             # Transformers.js wrapper for all-MiniLM-L6-v2
│   ├── redactor.ts             # Secret redaction before storage
│   └── hybrid-search.ts        # BM25 + vector fusion search
└── data/                       # gitignored
    └── retro.db                # SQLite database
```

### 10. Dependencies

```bash
bun add sqlite-vec @huggingface/transformers
```

Both packages have confirmed Bun support. `bun:sqlite` is a built-in Bun module requiring no installation. The `all-MiniLM-L6-v2` model is downloaded on first use and cached at `~/.cache/huggingface/` (~45 MB).

### 11. Scope Assessment

**Tier: F-lite** — Clear scope, documented requirements (this analysis), single domain (developer tooling in `.claude/skills/`). No new architecture concepts in the application itself. No cross-domain boundaries (this does not touch `apps/api`, `apps/web`, or `packages/`).

**Affected locations:**

| Location | Changes |
|----------|---------|
| `.claude/skills/retro/` | All new files — skill definition, scripts, library modules |
| `.gitignore` | Add `.claude/skills/retro/data/` to ignore the SQLite database |
| `CLAUDE.md` | Add `/retro` to the Available Skills table |

### 12. Developer Experience — /retro Skill Interface

This section describes what the developer actually sees when running `/retro`:

- **`/retro`** (default invocation): Shows a summary dashboard -- total sessions parsed, total findings by type, top 5 recurring blockers, and recent improvements.
- **`/retro --parse`**: Runs Phase 1 (mechanical parse). Displays progress per file and reports the total session count when complete.
- **`/retro --analyze`**: Runs Phase 2 (AI-powered analysis). Displays progress per session (e.g., "Analyzing session 42/758...") and summarizes findings extracted when complete.
- **`/retro --recap`**: Runs Phase 3 (recap). Generates a trend report for the last 30 days, including recurring blockers, improvements, and top suggestions.
- **`/retro --search "query"`**: Runs hybrid search against the findings database. Returns ranked findings with context, type, severity, and source session.
- All output is rendered as structured markdown in the terminal -- no external tools needed.

## Conclusions

1. **Claude Code CLI is the right AI engine** for classification. It avoids API key management entirely and uses the developer's existing authentication. Sequential processing (one session at a time) is simple and sufficient.

2. **sqlite-vec is viable** on Bun/WSL2/Linux. Alpha status is acceptable for developer tooling. The brute-force search algorithm handles our scale (&lt;10k vectors) without any performance concerns.

3. **The three-phase pipeline** cleanly separates zero-cost mechanical parsing (Phase 1) from AI-powered classification (Phase 2) and database-only trend analysis (Phase 3). Each phase is independently runnable and idempotent.

4. **Hybrid search** (0.7 vector + 0.3 BM25) is a proven approach from the 2ndBrain implementation. The combination of semantic similarity and keyword matching provides robust search across finding types.

5. **Privacy is handled pragmatically** with regex-based redaction. This is appropriate for a local-only tool — the transcripts themselves are already unredacted on disk.

6. **Manual trigger only** (`/retro` skill) is the right integration model. No hooks, no cron, no automation. The developer explicitly decides when to run retrospective analysis.

7. **All dependencies are confirmed** to work with Bun. No external services or API keys are required beyond the existing Claude Code authentication.

8. **The initial 758-session run is the largest risk**, estimated at 1-3 hours via sequential CLI processing. Error recovery via `processing_log` ensures partial runs are safe to interrupt and resume. The spec should mandate progress reporting and a `--limit N` flag for testing with smaller batches.

## Next Steps

- [ ] Create spec for #227 with file-level implementation details, function signatures, and test plan
- [ ] Add `.claude/skills/retro/data/` to `.gitignore`
- [ ] Implement Phase 1 (mechanical parse) first as a standalone deliverable — validates the SQLite + sqlite-vec setup with zero AI cost
- [ ] Implement Phase 2 (AI classification) with a small batch (10 sessions) to validate the Claude CLI integration and finding quality
- [ ] Implement Phase 3 (recap) and search after findings are populated
- [ ] Write the `SKILL.md` skill definition for `/retro`
- [ ] Add `/retro` to the Available Skills table in `CLAUDE.md`
- [ ] Validate embedding quality by spot-checking semantic search results against known related findings

## References

- [sqlite-vec GitHub](https://github.com/asg017/sqlite-vec) — SQLite vector search extension, API documentation
- [Transformers.js Documentation](https://huggingface.co/docs/transformers.js) — Local embeddings via `@huggingface/transformers`
- [all-MiniLM-L6-v2 Model Card](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) — Embedding model specifications
- [SQLite FTS5 Documentation](https://www.sqlite.org/fts5.html) — Full-text search extension
- [2ndBrain Knowledge Skill](https://github.com/MickaelV0/2ndbrain) — Prior art for hybrid search architecture
