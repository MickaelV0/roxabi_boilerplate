---
title: "Token Consumption Investigation + Complexity Scoring Rubric"
description: Phase 1 investigation — token hotspots, baseline metrics, and 1-10 complexity scoring rubric for tier determination.
---

**Status:** Complete — Feb 2026
**Issue:** #280 (Phase 1 of #251 AI Workflow Improvement Strategy)
**Tier:** S

## Context

The Roxabi boilerplate operates 9 agents, 15 skills, and a 4-stage pipeline (bootstrap, scaffold, review, promote). Over a 16-day period (2026-01-23 to 2026-02-07), the system ran **808 Claude Code sessions** with **680 hours of compute time**, producing **624 commits** across **6,572 messages**. Token costs are the primary operating expense. This investigation identifies where tokens are spent and establishes baselines for measuring future improvements.

**Data source:** Billing dashboard lacks per-session/per-agent granularity. All estimates use the estimation method: tool call counts x tokens per tool type, cross-referenced with session transcript analysis from the retro system (758 parsed sessions).

## Token Consumption by Agent

### Estimation Methodology

Token estimates are derived from:
1. **System prompt size** — measured line count per agent definition
2. **Typical file I/O** — file reads (input) and writes (output) per agent type, based on codebase patterns
3. **Tool call overhead** — ~50 tokens per tool invocation
4. **Context accumulation** — agents reading many files accumulate large input contexts

Estimates represent a single agent spawn (one invocation). Multiply by spawns-per-workflow for total workflow cost.

### Per-Agent Token Estimates

| Rank | Agent | Input Tokens | Output Tokens | Primary Cost Driver |
|------|-------|-------------|---------------|-------------------|
| 1 | security-auditor | 18,000-48,000 | 1,000-2,500 | Reads 40-100 files for audit scope |
| 2 | architect | 12,000-25,000 | 1,500-3,600 | Reads 30-50 files for cross-cutting analysis |
| 3 | frontend-dev | 8,500-15,000 | 2,000-4,500 | Reads 20-30 reference + pattern files |
| 4 | backend-dev | 7,500-14,000 | 2,000-4,500 | Reads 15-25 service + ORM pattern files |
| 5 | product-lead | 5,500-11,500 | 1,000-2,000 | Reads issues + specs + analyses |
| 6 | tester | 5,000-8,500 | 1,200-2,400 | Reads source + existing tests |
| 7 | devops | 4,000-8,000 | 1,500-3,600 | Reads config + CI/CD files |
| 8 | fixer | 3,500-7,500 | 2,000-4,500 | Targeted scope but high output |
| 9 | doc-writer | 3,000-6,000 | 1,000-1,800 | Focused doc scope |

### Per-Workflow Token Estimates

| Workflow | Agent Spawns | Input Tokens | Output Tokens | Total |
|----------|-------------|-------------|---------------|-------|
| **Review** (full) | 7 reviewers + 1-3 fixers | 155,000-190,000 | 20,000-35,000 | **175,000-225,000** |
| **Scaffold** (F-full) | 4-6 domain agents | 90,000-130,000 | 12,000-24,000 | **102,000-154,000** |
| **Bootstrap** | 2-4 expert reviewers | 25,000-50,000 | 5,000-10,000 | **30,000-60,000** |
| **Promote** | 0 (direct orchestration) | 5,000-10,000 | 2,000-4,000 | **7,000-14,000** |

**Full pipeline (bootstrap + scaffold + review):** 307,000-439,000 tokens per F-full feature.

## Top 5 Token Hotspots

### Hotspot 1: Review Workflow (175-225k tokens)

**Why:** Spawns 7 fresh review agents (security-auditor, architect, product-lead, tester, + conditional domain agents), each reading the full PR diff and relevant codebase context. Then spawns 1-3 fixers for accepted findings.

**Impact:** Single most expensive workflow stage. The security-auditor alone consumes 18-48k input tokens per spawn due to broad codebase scanning.

**Mitigation opportunity:** Scope security-auditor to changed files + their direct dependencies rather than full codebase audit. Potential savings: 50-70% of security-auditor input tokens.

### Hotspot 2: Security-Auditor File Scanning (18-48k per spawn)

**Why:** Read-only agent that surveys 40-100 files per audit. No Write/Edit tools, so all cost is input. Scans entire module trees rather than targeted paths.

**Impact:** Single highest-cost agent per spawn. Spawned in both scaffold (F-full) and review workflows.

**Mitigation opportunity:** File-path filtering based on PR changed files. Limit audit scope to changed files + 1 hop of imports.

### Hotspot 3: Scaffold F-full Team Spawning (102-154k tokens)

**Why:** Spawns 4-6 concurrent agents (tester, frontend-dev, backend-dev, architect, devops, security-auditor as needed). Each agent independently reads reference files, standards docs, and codebase patterns.

**Impact:** Redundant file reads across agents (e.g., all agents may read the same standards doc). No shared context between team members.

**Mitigation opportunity:** Pre-compute a shared context summary (key patterns, relevant file excerpts) and inject into each agent prompt. Reduces per-agent file reads by 30-50%.

### Hotspot 4: Architect Cross-Cutting Analysis (12-25k per spawn)

**Why:** Reads 30-50 files spanning architecture docs, existing patterns, and design decisions. Spawned in both bootstrap (expert review) and scaffold (F-full planning).

**Impact:** Second-highest per-spawn cost. Often spawned twice per feature lifecycle (bootstrap review + scaffold planning).

**Mitigation opportunity:** Cache architecture context between bootstrap and scaffold phases. If bootstrap architect already reviewed the codebase, pass findings to scaffold architect instead of re-reading.

### Hotspot 5: Duplicate Standards Doc Reads Across Agents

**Why:** Multiple agents independently read the same standards documents (frontend-patterns.mdx, backend-patterns.mdx, testing.mdx, code-review.mdx) at the start of their tasks.

**Impact:** Estimated 2,000-4,000 wasted tokens per agent per shared doc read, multiplied by 4-6 agents = 8,000-24,000 tokens of redundant reads per scaffold.

**Mitigation opportunity:** Include relevant standards excerpts in agent task prompts rather than having each agent discover and read them independently.

## Baseline Metrics

### Session Count Per Feature (Last 3 Features)

| Feature | Issue | Sessions | Agents Spawned | Tier |
|---------|-------|----------|---------------|------|
| User avatar + navbar integration | #242 | 3-4 | 4 (frontend-dev, backend-dev, tester, doc-writer) | F-lite |
| Auth flow review | #200 | 2-3 | 3 (product-lead, architect, doc-writer) | S (analysis only) |
| AI workflow strategy | #251 | 2-3 | 2 (product-lead, doc-writer) | S (analysis + spec) |

**Average:** 2.5-3.3 sessions per feature.

### Lead Intervention Count Per Scaffold (Last 3 Scaffolds)

| Scaffold | Issue | Lead Interventions | Type |
|----------|-------|-------------------|------|
| Build steps (types + config) | #246 | 2 | Scope clarification, test strategy |
| Documentation onboarding | #226 | 1 | Structure decision |
| CI/CD pipeline hardening | #222 | 3 | Config conflicts, deployment strategy, rollback |

**Average:** 2.0 interventions per scaffold.

**Intervention types observed:**
- Scope clarification (most common)
- Strategy/approach decisions
- Conflict resolution between agent outputs
- Quality gate failures requiring human judgment

### Process Metrics (from Retrospective)

| Metric | Value | Source |
|--------|-------|--------|
| Total sessions (16-day window) | 808 | claude-code-usage-insights.mdx |
| Compute hours | 680 | claude-code-usage-insights.mdx |
| Task completion rate | 88% | claude-code-usage-insights.mdx |
| Skill invocation success rate | 53% | claude-code-ai-agents-recommendations.mdx |
| Wrong approach → wasted cycles | 27 instances | claude-code-usage-insights.mdx |
| Multi-agent parallel execution (largest) | 19 fixes / 4 agents / 30 files | claude-code-usage-insights.mdx |

## Complexity Scoring Rubric (1-10)

Inspired by TaskMaster.dev's complexity analysis. Maps to existing S / F-lite / F-full tiers.

### Scoring Factors

| Factor | Weight | 1 (Low) | 5 (Medium) | 10 (High) |
|--------|--------|---------|------------|-----------|
| **Files touched** | 20% | 1-3 files | 5-10 files | 15+ files |
| **Technical risk** | 25% | Known patterns, no new tech | New library or pattern in 1 domain | New architecture, untested approach |
| **Architectural impact** | 25% | Single module, no exports | Shared types or 2 modules | Cross-domain, new abstractions, API contracts |
| **Unknowns count** | 15% | 0 unknowns, fully documented | 1-2 open questions | 3+ unknowns, needs investigation |
| **Domain breadth** | 15% | 1 domain (FE or BE only) | 2 domains (FE + BE) | 3+ domains (FE + BE + infra + docs) |

### Calculation

```
complexity = round(
  files_score      × 0.20 +
  risk_score       × 0.25 +
  arch_score       × 0.25 +
  unknowns_score   × 0.15 +
  domains_score    × 0.15
)
```

Each factor scored 1-10. Weighted sum produces the final score (1-10).

### Tier Mapping

| Score | Tier | Process | Agent Mode |
|-------|------|---------|-----------|
| 1-3 | **S** (Quick Fix) | Worktree + direct implementation + PR | Single session, no agents |
| 4-6 | **F-lite** (Feature lite) | Worktree + subagents + /review | Task tool subagents (1-2 domain + tester) |
| 7-10 | **F-full** (Feature full) | Bootstrap + worktree + agent team + /review | TeamCreate (3+ agents, test-first) |

### Scoring Examples

| Feature | Files | Risk | Arch | Unknowns | Domains | Score | Tier |
|---------|-------|------|------|----------|---------|-------|------|
| Fix typo in README | 1 | 1 | 1 | 1 | 1 | **1** | S |
| Add env var validation | 3 | 2 | 1 | 1 | 1 | **2** | S |
| New API endpoint (CRUD) | 6 | 3 | 3 | 1 | 2 | **3** | S |
| DiceBear avatar integration | 8 | 4 | 5 | 2 | 3 | **4** | F-lite |
| Rate limiting middleware | 5 | 5 | 5 | 2 | 2 | **4** | F-lite |
| Multi-tenant RLS | 10 | 7 | 8 | 3 | 3 | **7** | F-full |
| Auth system overhaul | 15 | 8 | 9 | 4 | 4 | **8** | F-full |
| Bootstrap enrichment (#281) | 12 | 6 | 7 | 3 | 3 | **6** | F-full* |

*#281 scores 6 but is classified F-full due to unclear requirements and architectural decisions.

**Important:** The rubric is a guide, not a gate. Human judgment overrides the score. A score of 6 may be F-lite (clear mechanical changes) or F-full (design decisions needed). The score informs the conversation, not the decision.

## Recommendations

### Immediate (Phase 1 outputs)

1. **Integrate complexity scoring into bootstrap Step 0** — Score every issue before choosing a tier
2. **Add scoring to issue-triage** — Record complexity score alongside size/priority on the project board
3. **Establish measurement protocol** — Track sessions per feature and interventions per scaffold going forward

### Future Phases (informed by this investigation)

1. **Phase 7 (#286):** Use hotspot data to prioritize Sonnet candidates — doc-writer, fixer, tester, devops are lowest-cost agents and best candidates for Sonnet
2. **Phase 3 (#282):** Micro-task generation should factor in complexity score to determine task granularity
3. **Phase 4 (#283):** Shared task lists should reduce redundant file reads (Hotspot 5)

### Gaps Not Covered by Current Phases

Two hotspot mitigations are **not** addressed by any existing phase:

1. **Pre-computed shared context (Hotspot 5):** No phase explicitly tackles injecting standards excerpts or reference patterns into agent prompts to avoid redundant file reads. Phase 4 (#283) adds shared task lists for coordination but not shared context for file I/O reduction. **Recommendation:** Add a sub-task to Phase 4 or create a follow-up issue for "shared context injection" — pre-compute key file excerpts and include them in agent task prompts during scaffold Step 5.

2. **Architect context caching (Hotspot 4):** No phase addresses passing bootstrap architect findings to scaffold architect. The architect is spawned independently in both phases, re-reading 30-50 files each time. **Recommendation:** Add to Phase 3 (#282) — when generating the committed plan artifact, include architect findings from bootstrap so the scaffold architect receives them as context rather than re-discovering them.

## Open Questions

- **Billing granularity:** Per-session billing data would validate these estimates. Until available, re-run estimation quarterly.
- **Hotspot 1 mitigation:** Scoping security-auditor requires changes to the review skill — deferred to a future phase.
- **Baseline stability:** 3 features is a small sample. Capture 5-7 more data points before using baselines for A/B comparison.
