---
title: "Optional Investigation Step in Bootstrap Pipeline"
description: Spec for adding an optional investigation/PoC step (Gate 1.5) to the bootstrap pipeline between analysis approval and spec writing
---

## Context

The bootstrap pipeline (`/bootstrap`) goes from analysis to spec in a single jump. When the analysis identifies technical uncertainty — untested libraries, competing approaches without clear data, unknown API behavior — the spec is forced to commit to a solution based on assumptions. If those assumptions break during `/scaffold`, the team discovers it mid-implementation, causing rework.

This spec adds an **optional investigation step** between Gate 1 (Analysis) and Gate 2 (Spec). When uncertainty is detected, the user can choose to spike a solution, validate assumptions with real code, and feed concrete findings into the spec.

**Promoted from:** [Bootstrap Investigation Step — Analysis](../analyses/bootstrap-investigation-step)

**GitHub issue:** #229

## Goal

Allow the bootstrap pipeline to validate technical assumptions before committing to a spec, so that specs are based on evidence rather than guesses. The step must be optional (zero friction when not needed), user-controlled, and produce structured findings that enrich the analysis document.

## Users &amp; Use Cases

| User | Scenario |
|------|----------|
| **Human lead** | Approves analysis, confirms whether to investigate, reviews findings, approves spec |
| **Bootstrap orchestrator** (Main Claude) | Detects uncertainty signals, proposes investigation, defines scope, spawns subagents, appends findings |
| **Domain expert subagent** | Executes the spike on a throwaway branch, tests approaches, reports findings |

**Typical workflow:**
1. Human runs `/bootstrap "feature idea"`
2. Analysis is produced and approved (Gate 1)
3. Orchestrator detects "needs testing" and "2 competing approaches" in the analysis
4. Orchestrator asks: "Investigation suggested — investigate or skip?"
5. Human chooses "Investigate"
6. Orchestrator proposes question + done criteria, human confirms
7. Subagent spikes on `spike/` branch, reports findings
8. Human reviews findings, orchestrator appends to analysis
9. Spike branch deleted, pipeline continues to Gate 2 (Spec)

## Expected Behavior

### Happy path

**Step 1 — Uncertainty detection** (after Gate 1 approval)

The orchestrator scans the approved analysis for uncertainty signals:

| Category | Signal patterns |
|----------|----------------|
| **Explicit uncertainty markers** | "needs testing", "unclear if", "to be validated", "unknown performance", "requires investigation", trade-off sections without a concluded decision |
| **Multiple competing approaches** | 2+ approaches listed without a clear winner, or a recommendation "pending validation" |
| **External dependency risk** | Reliance on a third-party API, library, or service not yet tested in this project |

Detection is a **heuristic** — these phrases can appear in normal analytical writing. False positives are handled by the user choosing "Skip." If no signals are detected, the pipeline skips directly to "Ensure GitHub Issue."

**Step 2 — User confirmation**

If signals are detected, present via `AskUserQuestion`:

```
"The analysis contains technical uncertainty: {detected signals summary}.
Would you like to investigate before writing the spec?"

Options:
- Investigate — Spike the uncertain areas before speccing
- Skip to spec — Proceed directly to Gate 2
```

If the user skips, continue to "Ensure GitHub Issue." If the user confirms, proceed to Step 3.

**Step 3 — Scope definition**

The orchestrator drafts and presents to the user via `AskUserQuestion`:

1. **Investigation question(s)** — Specific technical question(s) to answer
2. **Done criteria per question:**
   - **Success conditions** — What validates the approach
   - **Failure conditions** — What invalidates the approach

The user reviews and may adjust before the spike begins.

**Example:**

```
Question: "Can sqlite-vec run on Bun/WSL2 with acceptable query latency?"

Done criteria:
- Success: sqlite-vec installs, 10k vector queries complete in <50ms
- Failure: sqlite-vec fails to install on Bun, or query latency >500ms
```

**Step 4 — Create spike branch**

```bash
# Bare-text bootstrap (no issue number yet):
git worktree add ../roxabi-spike-{slug} -b spike/{slug} staging
cd ../roxabi-spike-{slug}
bun install

# --issue N bootstrap (issue number available):
git worktree add ../roxabi-spike-XXX -b spike/XXX-{slug} staging
cd ../roxabi-spike-XXX
bun install
```

The `bun install` step is required so the subagent can import and test libraries in the spike worktree. No `db:branch:create` is needed — spikes are for feasibility testing, not full feature development.

**Step 5 — Execute investigation**

Spawn the appropriate domain expert subagent(s) via the `Task` tool:

| Uncertainty domain | Subagent |
|-------------------|----------|
| Architecture, feasibility, library evaluation | `architect` |
| Backend API, database, ORM | `backend-dev` |
| Frontend rendering, UI library, browser APIs | `frontend-dev` |
| CI/CD, deployment, infrastructure | `devops` |
| Cross-cutting or unclear | `architect` (default) |

**Subagent prompt template:**

```
You are investigating a technical question for the bootstrap pipeline.

**Analysis document:** {path to analysis}
**Working directory:** {spike worktree path}

## Investigation Question

{question text}

## Done Criteria

- **Success:** {success conditions}
- **Failure:** {failure conditions}

## Instructions

1. Read the analysis document for full context.
2. Work in the spike worktree to test the approach.
3. If the initial approach FAILS the done criteria, pivot to alternative
   approaches listed in the analysis. Do NOT invent new approaches.
4. **Self-monitoring:** If you reach 15 tool calls without meeting done
   criteria, STOP and return your partial findings immediately.
5. Report findings using this structure:

   **Approach tested:** {what you tested}
   **Result:** Success | Failure | Partial
   **Findings:** (bullet points with key observations, data, errors)
   **Recommendation:** {what this means for the spec}

If you tested multiple approaches, report each separately.
```

**Self-monitoring safeguard:** The 15-turn limit is an instruction to the subagent, not orchestrator-side monitoring. The `Task` tool is fire-and-wait — the orchestrator cannot interrupt mid-execution. The subagent is responsible for tracking its own tool-call count and returning partial findings if stuck.

**Cross-domain investigations:** Split questions by domain, spawn parallel subagents with separate prompts. The orchestrator merges findings into a single "Investigation Findings" section with sub-headings per question.

**Step 6 — Collect and review findings**

The subagent returns findings via the `Task` tool return value. If findings are too large, the subagent writes them to a file in the spike worktree and returns the path.

The orchestrator presents the findings summary to the user via `AskUserQuestion`:

```
"Investigation findings for: {question}
Result: {Success | Failure | Partial}

Key findings:
- {finding 1}
- {finding 2}

Recommendation: {what this means for the spec}"

Options:
- Approve findings — Append to analysis as-is
- Edit findings — I have corrections before appending
- Discard findings — Do not append, proceed without
```

**"Edit findings" flow:** If the user chooses "Edit findings," the orchestrator asks for their corrections via `AskUserQuestion` (free-text input). The orchestrator applies the corrections to the findings text in memory, then re-presents the updated findings for final approval. This is a text-editing loop, not a re-run of the spike. "Edit findings" means correcting the written findings, not re-investigating.

**Step 7 — Append findings to analysis**

Append an "Investigation Findings" section to the analysis document (see [Findings Template](#findings-template)).

**Step 8 — Clean up spike branch**

```bash
# Guard: verify the analysis file was modified before cleanup
# (orchestrator checks the analysis document contains "## Investigation Findings"
#  with substantive content — not just the heading)

# Return to the primary project working directory
git worktree remove ../roxabi-spike-{slug}
git branch -D spike/{slug}
```

> **Note on `git branch -D`:** This is an intentional force-delete of a throwaway branch. Subagents may commit code to the spike branch during testing — those commits are intentionally discarded. This is a pre-approved destructive operation within the investigation step.

If the findings were not successfully written (guard fails), preserve the branch and inform the user.

**Step 9 — Continue pipeline**

- If investigation found a feasible approach → continue to "Ensure GitHub Issue" → Gate 2
- If all approaches failed → see [Infeasibility](#infeasibility) edge case

### Findings template

The orchestrator appends this to the analysis document:

```mdx
## Investigation Findings

> Investigation conducted on {YYYY-MM-DD} by {subagent_type} subagent.
> Branch: `spike/{slug}` (deleted after findings extraction).

### Question 1: {investigation question}

**Approach tested:** {what was tested}

**Result:** {Success | Failure | Partial}

**Findings:**
- {Key finding 1}
- {Key finding 2}
- {Performance data, compatibility notes, etc.}

**Recommendation:** {What this means for the spec}
```

### Edge cases

#### Infeasibility

If the subagent's initial approach fails, it **pivots** to alternative approaches mentioned in the analysis (no inventing new ones). If **all** listed approaches fail:

1. Findings (including all failure data) are appended to the analysis
2. Spike branch is cleaned up
3. Orchestrator presents to user via `AskUserQuestion`:

```
"Investigation found all approaches infeasible. Findings have been appended to the analysis.
How would you like to proceed?"

Options:
- Revise analysis — Return to Gate 1 with new direction
- Proceed to spec anyway — Accept the risk and write the spec with current findings
- Abandon — Stop the bootstrap pipeline
```

**"Revise analysis" re-entry:** The orchestrator re-enters the Gate 1a → 1b → 1c cycle. The user provides new direction (e.g., "try a different library"), the orchestrator revises the analysis (keeping the investigation findings as evidence of what didn't work), re-runs expert review, and re-presents for approval. After re-approval, the pipeline proceeds normally (and may trigger investigation again if new uncertainties are present). There is no limit on revision cycles — the user can always choose "Abandon" to exit.

#### No signals detected

If the auto-detection finds no uncertainty signals, the investigation step is completely skipped. The user sees no prompt and the pipeline proceeds directly to "Ensure GitHub Issue."

#### User skips investigation

If signals are detected but the user chooses "Skip to spec," no spike branch is created and the pipeline proceeds normally.

#### Subagent reaches 15-turn self-monitoring limit

The subagent returns partial findings (as instructed in its prompt). The orchestrator presents partial findings to the user via `AskUserQuestion` and asks: spawn a new subagent to continue the investigation, or stop with partial findings?

#### Findings extraction fails

If the analysis file was not modified (guard check), the spike branch is preserved. The user is informed and can manually extract findings or retry.

#### Multiple uncertainties across domains

Spawn parallel subagents (one per domain). Each receives only its domain's questions. The orchestrator merges findings into a single "Investigation Findings" section with sub-headings per question.

#### Bootstrap started with `--spec N`

Investigation is not applicable — `--spec` skips Gate 1 entirely. Investigation only runs after a Gate 1 approval, regardless of whether the spec text itself contains uncertainty language.

#### Branch naming collision

If two parallel bootstraps attempt to create `spike/` branches with the same slug, the `git worktree add` command will fail with a git error. The orchestrator should present the error to the user and suggest a disambiguated slug (e.g., append a short suffix).

## Constraints

- **Must be purely optional** — Zero friction added to bootstraps without uncertainty
- **No new agent types** — Uses existing domain expert subagents (architect, backend-dev, frontend-dev, devops)
- **No new issue board status** — Investigation stays within "Analysis" status
- **Spike branches are throwaway** — No code survives; only findings are persisted
- **User controls every decision** — Confirm investigation, confirm scope, review findings, choose next step on failure

## Non-goals

- **Automated investigation without user confirmation** — The user always decides whether to investigate
- **Persisting spike code** — Spike branches are always deleted; no option to keep or merge
- **New issue board status** — Investigation is a research activity within the "Analysis" phase, not a separate lifecycle state
- **Investigation for Tier S or F-lite** — This step is part of the bootstrap pipeline, which only runs for Tier F-full tasks

## Technical Decisions

### Findings as analysis addendum (not standalone document)

Appending to the existing analysis keeps all pre-spec research in one place, avoids creating a third document type with its own `meta.json` wiring, and makes the full reasoning chain readable in sequence. The spec can cite specific findings sections.

### Heuristic detection over structured flags

Using keyword/pattern detection rather than structured metadata (e.g., a `needs_investigation: true` frontmatter field) is intentional. It works on any analysis document without requiring authors to explicitly flag uncertainty. False positives are cheap (user just skips).

### User-confirmed scope over orchestrator-inferred scope

The orchestrator drafts the investigation question and done criteria, but the user must confirm. This prevents the orchestrator from answering the wrong question — the cost of a bad investigation question is a wasted spike.

### 15-turn soft safeguard via subagent self-monitoring

Hard timeboxes are arbitrary and may cut off investigation right before a breakthrough. The `Task` tool is fire-and-wait — the orchestrator cannot interrupt a subagent mid-execution. Instead, the 15-turn limit is an instruction in the subagent prompt: the subagent tracks its own tool-call count and returns partial findings if it reaches 15 calls without meeting done criteria. This is a best-effort safeguard — subagents may occasionally overshoot, but the instruction provides a reliable soft limit.

### Spike branch naming uses analysis slug

Since the investigation runs before "Ensure GitHub Issue" (the issue may not exist yet for bare-text bootstraps), the spike branch uses the analysis slug: `spike/{analysis-slug}`. If an issue number is available, it uses `spike/XXX-slug` for consistency.

## Implementation

### Files to modify

| File | Changes |
|------|---------|
| `.claude/skills/bootstrap/SKILL.md` | Add investigation step between Gate 1c approval and "Ensure GitHub Issue" section |
| `docs/processes/dev-process.mdx` | Add `spike/` to naming conventions table; mention investigation in Phase 1 flow |
| `CLAUDE.md` | Add `spike/` to branch naming; update bootstrap flow if referenced |

### Changes to `.claude/skills/bootstrap/SKILL.md`

Add a new section **"Gate 1.5: Investigation (Optional)"** between Gate 1c (User Approval) and "Ensure GitHub Issue":

```markdown
## Gate 1.5: Investigation (Optional)

> Runs only after Gate 1 approval. Skipped when using `--spec`.

### 1.5a. Detect Uncertainty Signals

Read the approved analysis and check for uncertainty signals:
- Explicit markers: "needs testing", "unclear if", "to be validated", "unknown performance", "requires investigation"
- Competing approaches: 2+ approaches without a clear winner
- External dependency: untested third-party API, library, or service

If no signals detected → skip to "Ensure GitHub Issue."

### 1.5b. User Confirmation

Present via AskUserQuestion:
> "The analysis contains technical uncertainty: {signals}. Investigate before speccing?"
> - Investigate
> - Skip to spec

If user skips → skip to "Ensure GitHub Issue."

### 1.5c. Define Scope

Draft investigation question(s) and done criteria. Present to user via AskUserQuestion for confirmation.

### 1.5d. Execute Spike

1. Create spike branch and install dependencies:
   ```bash
   git worktree add ../roxabi-spike-{slug} -b spike/{slug} staging
   cd ../roxabi-spike-{slug} && bun install
   ```
2. Spawn domain expert subagent(s) via Task tool using the subagent prompt template (see spec for full template). Key instructions:
   - Analysis path, question(s), done criteria, spike worktree path
   - Pivot to alternatives from analysis if initial approach fails
   - Self-monitoring: stop and return partial findings after 15 tool calls
3. For cross-domain investigations, spawn parallel subagents and merge findings into sub-headings per question.

### 1.5e. Review Findings

Present findings to user via AskUserQuestion:
- **Approve** — Append as-is
- **Edit** — Collect user corrections (text only, not re-investigation), re-present
- **Discard** — Proceed without appending

### 1.5f. Append and Cleanup

1. Append approved findings to analysis as "## Investigation Findings" section
2. No `meta.json` update needed — findings are appended to the existing analysis in place
3. Guard: verify findings written (analysis contains "## Investigation Findings" with content)
4. Delete spike branch and worktree (`git branch -D` is pre-approved for throwaway spikes)
5. If all approaches failed → AskUserQuestion: revise / proceed / abandon
6. Continue to "Ensure GitHub Issue"
```

### Changes to `docs/processes/dev-process.mdx`

In the **Naming Conventions** table, add:

```
| Branch | `spike/XXX-slug` | `spike/123-sqlite-vec` |
```

In the **Phase 1 — Assessment** section, add a note after the paragraph beginning "Once spec is approved, product-lead returns control to Main Claude" (inside the F-full description):

```
> **Investigation (optional):** If the analysis reveals technical uncertainty, the orchestrator may suggest spiking a solution before writing the spec. See the bootstrap skill (Gate 1.5) for details.
```

### Changes to `CLAUDE.md`

Verify `CLAUDE.md` against the following before deciding to skip:
- The worktree command in Rule 6 uses `feat/XXX-slug` as the example — `spike/` branches do not follow that pattern (they use `spike/{slug}` or `spike/XXX-slug`) and are created by the bootstrap skill, not the user. No change needed to Rule 6.
- The bootstrap flow description in the skills table is high-level ("Orchestrates from raw idea to approved spec through two validation gates") and does not enumerate sub-steps. No change needed.
- The naming conventions in the Routing Decision Tree reference branch creation but not branch prefixes. No change needed.

**Conclusion:** No changes to `CLAUDE.md` are required. The `spike/` prefix is documented in `dev-process.mdx` (naming conventions) and the bootstrap `SKILL.md` (where it is used). This reverses the analysis's initial assumption that `CLAUDE.md` needed updating — upon inspection, the file does not enumerate branch prefixes at the required level of detail.

## Success Criteria

- [ ] `/bootstrap` detects uncertainty signals in approved analyses and offers investigation via `AskUserQuestion`
- [ ] When no uncertainty signals are detected, the pipeline proceeds directly to "Ensure GitHub Issue" with no investigation prompt shown
- [ ] When the user chooses "Skip to spec," no spike branch is created and the pipeline proceeds normally
- [ ] Investigation scope (question + done criteria) is presented to user for confirmation before spike starts
- [ ] Spike branch (`spike/` prefix) is created with `bun install` and domain subagent executes successfully in the worktree
- [ ] Subagent self-monitors and returns partial findings if stuck after 15 tool calls
- [ ] Investigation findings are appended to the analysis document after user review
- [ ] Spike branch and worktree are cleaned up after findings extraction (with deletion guard)

## Open Questions

- **Signal detection granularity**: Should the heuristic use simple string matching or a more nuanced LLM-based classification of uncertainty? String matching is simpler but may have a higher false-positive rate. Can be iterated on after initial implementation.
- **Multi-round investigation**: If the user reviews findings and says "test one more thing," should the investigation support multiple rounds on the same spike branch? Current spec assumes a single round. Could be a v2 enhancement.
- **False-positive rate validation**: Before shipping, run the signal detection against 2-3 real analysis documents from `analyses/` to verify the heuristic doesn't over-trigger. If most analyses fire the prompt, the detection threshold needs tightening.
