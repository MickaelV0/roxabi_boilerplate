---
title: "Session Intelligence — /retro Skill"
description: A local session intelligence system that parses Claude Code transcripts, classifies findings via AI, stores them with hybrid search, and provides trend analysis through a /retro skill
---

## Context

Roxabi Boilerplate has accumulated 758 Claude Code session transcripts as JSONL files in `~/.claude/projects/-home-mickael-projects-roxabi-boilerplate/`. These transcripts contain actionable intelligence — recurring blockers, praise for patterns that work well, suggestions from Claude, and nitpicks about code style or process — but none of it is systematically captured or analyzed today.

This feature builds a **session intelligence system** exposed through a `/retro` skill. It mechanically parses all session transcripts into a local SQLite database, uses AI-powered classification (via the Claude Code CLI) to extract and categorize findings, provides trend analysis and semantic search, and requires no external API keys or services.

**Promoted from:** [Session Intelligence Analysis](../analyses/227-session-intelligence)

**Dependencies:**

- `bun:sqlite` (built-in) for structured storage
- `sqlite-vec@0.1.7-alpha.2` for vector search (pinned to tested version)
- `@huggingface/transformers@^3.0.0` (Transformers.js v3 for Bun compatibility) for local embeddings
- Claude Code CLI (`claude`) for AI-powered classification — uses developer's existing authentication
- Existing Claude Code session transcripts at `~/.claude/projects/-home-mickael-projects-roxabi-boilerplate/`

## Goal

Enable developers to systematically extract, search, and analyze intelligence from their Claude Code session history — identifying recurring blockers, tracking improvements over time, surfacing actionable suggestions, and informing process/standards evolution — all through a manually triggered `/retro` skill with zero external dependencies.

## Users &amp; Use Cases

**Primary user:** Developer (solo) using Claude Code with Roxabi Boilerplate.

| Use Case | Workflow |
|----------|----------|
| Initial setup | Developer runs `/retro --setup` to download the embedding model, verify sqlite-vec, and create the database |
| Populate session data | Developer runs `/retro --parse` to mechanically extract metadata from all JSONL transcripts into SQLite |
| Classify findings | Developer runs `/retro --analyze` to send sessions through Claude CLI for AI-powered finding extraction |
| Test classification | Developer runs `/retro --analyze --limit 10` to process a small batch and validate finding quality |
| View dashboard | Developer runs `/retro` to see a summary: total sessions, findings by type, top 5 blockers, recent improvements |
| Search findings | Developer runs `/retro --search "authentication"` to find semantically relevant findings across all sessions |
| Filter search | Developer runs `/retro --search "auth" --type blocker` to narrow results to blockers only |
| Trend analysis | Developer runs `/retro --recap` to generate a 30-day trend report of blockers, improvements, and regressions |
| Custom period recap | Developer runs `/retro --recap --period weekly` for a weekly trend report |
| Re-analyze sessions | Developer runs `/retro --reanalyze &lt;session-id>` after improving the classification prompt |
| Re-analyze all | Developer runs `/retro --reanalyze all` to rebuild all findings from scratch |

## Expected Behavior

### `/retro` — Dashboard (default)

1. Query the database for aggregate statistics
2. Display a structured markdown summary:
   - Total sessions parsed / total sessions available
   - Total findings by type (praise, blocker, suggestion, nitpick)
   - Top 5 recurring blockers (grouped by tag or content similarity)
   - Recent improvements (last 7 days of praise findings)
3. If the database does not exist, display: "No retro database found. Run `/retro --setup` to get started."
4. If no sessions have been analyzed yet, display parse/analyze counts and prompt the user to run `--parse` then `--analyze`

### `/retro --setup` — First-time initialization

1. Check for `bun:sqlite` availability (built-in, always present)
2. Attempt to load `sqlite-vec` extension — if missing, display install command: `bun add sqlite-vec@0.1.7-alpha.2`
3. Download `all-MiniLM-L6-v2` embedding model via Transformers.js (~45 MB, cached at `~/.cache/huggingface/`)
4. Display download progress
5. Create `.claude/skills/retro/data/` directory
6. Create `retro.db` with the full schema (tables, FTS5 virtual table, vec0 virtual table, triggers)
7. Verify by running a test embedding and a test vector insert
8. Display: "Setup complete. Run `/retro --parse` to import session transcripts."

### `/retro --parse` — Phase 1: Mechanical parse

1. Scan `~/.claude/projects/-home-mickael-projects-roxabi-boilerplate/` for `.jsonl` files
2. Cross-reference with `sessions-index.json` for metadata enrichment (summaries, dates, message counts). If `sessions-index.json` does not exist or is malformed, proceed with JSONL-only parsing. Metadata fields that come exclusively from the index (summary) will be NULL. Log a warning: "sessions-index.json not found or invalid — proceeding with limited metadata."
3. For each session file not already in the `sessions` table:
   - Parse the JSONL line by line
   - Extract: session ID, project path, git branch, first prompt, summary, message count, timestamps, duration
   - Insert into `sessions` table
   - Log to `processing_log` with `phase: 'parse'`, `status: 'success'`
4. Display progress: "Parsing session 42/758..."
5. On completion, display: "Parsed {N} new sessions ({M} already existed, {K} skipped). Total: {T} sessions."
6. **Idempotent:** Re-running skips sessions already present in the database

### `/retro --analyze` — Phase 2: AI-powered classification

1. Query sessions where `analyzed_at IS NULL`
2. For each unanalyzed session, sequentially:
   - Read the session's JSONL transcript
   - For very large sessions (&gt;10 MB or &gt;500 messages), truncate context to first 50 + last 50 messages to avoid token limits
   - Send extracted data to Claude CLI: `claude -p --output-format json`
   - The prompt instructs Claude to return a JSON array of findings, each with: `type`, `content`, `context`, `severity`, `tags`
   - Double JSON parse: outer CLI wrapper `{"result": "...", "session_id": "..."}`, inner findings JSON array
   - Apply secret redaction to each finding's `content` and `context` fields before storage
   - Insert findings into `findings` table (FTS5 triggers keep the search index in sync)
   - Generate embeddings for each finding via Transformers.js and insert into `finding_embeddings`
   - Set `analyzed_at` on the session record
   - Log to `processing_log` with `phase: 'analyze'`, `status: 'success'`
3. Display progress: "Analyzing session 42/758..."
4. On completion, display: "Analyzed {N} sessions. Extracted {F} findings ({P} praise, {B} blockers, {S} suggestions, {K} nitpicks)."
5. **Error recovery:** If the CLI call fails or JSON parsing fails for a session, log as `status: 'error'` with the error message, skip that session, and continue. The raw CLI output (including rate-limit messages, auth errors, or safety refusals) is preserved in `error_message` for debugging. Re-running resumes from the next unanalyzed session.
6. **Interruptible:** Ctrl+C is safe. Already-analyzed sessions are committed. Resume by re-running the command.

### `/retro --analyze --limit N` — Limited analysis

1. Same as `--analyze` but process only the first N unanalyzed sessions
2. Useful for testing classification quality with a small batch before committing to the full 1-3 hour initial run
3. Display: "Analyzing {N} sessions (limited run)..."

### `/retro --recap` — Phase 3: Trend report

1. Query findings from the last 30 days (default period)
2. Generate a structured markdown report:
   - **Blocker trends:** Most frequent blockers, grouped by tags. Highlight new vs. recurring.
   - **Improvements:** Praise findings indicating positive patterns. Track what is working well.
   - **Regressions:** Blockers (by tag) that appeared in an earlier period, were absent in a subsequent period, and reappeared in the current period. Detection is based on tag presence/absence across time windows — no explicit "resolved" status is needed.
   - **Top suggestions:** Most common suggestions, ranked by frequency.
   - **Process evolution:** How finding distribution has changed over the period.
3. Display the report in the terminal as formatted markdown
4. **Cost:** Zero — database queries only, no AI calls

### `/retro --recap --period weekly|monthly` — Configurable period

1. `weekly`: Last 7 days of findings
2. `monthly`: Last 30 days of findings (same as default)
3. Same report structure as above, scoped to the chosen period

### `/retro --search "query"` — Hybrid search

1. Generate an embedding for the query string using Transformers.js
2. Run vector search against `finding_embeddings` (weight: 0.7)
3. Run BM25 search against `findings_fts` (weight: 0.3)
4. Fuse results using weighted rank fusion
5. Display top results with: finding content, context snippet, type badge, severity, tags, source session ID, and date
6. Results are ordered by fused score (descending). Maximum 20 results returned by default. Each result displays:
```
[blocker] (high) authentication timeout during OAuth callback
  Tags: auth, oauth, timeout
  Session: abc123 | 2026-01-15
  Context: User asked to debug OAuth flow...
```
7. If no results found, display: "No findings match your query."

### `/retro --search "query" --type blocker` — Filtered search

1. Same as `--search` but apply a pre-filter on finding type before ranking
2. Supported type values: `praise`, `blocker`, `suggestion`, `nitpick`
3. Filters are applied to both the vector and BM25 candidate sets before fusion

### `/retro --reanalyze &lt;session-id>` — Re-analyze a single session

1. Look up the session by ID
2. Clear `analyzed_at` on the session record
3. Delete all findings for that session from `findings` (FTS5 triggers handle index cleanup)
4. Delete all corresponding embeddings from `finding_embeddings`
5. Re-run Phase 2 for that single session
6. Display: "Re-analyzed session {id}. Extracted {N} findings."

### `/retro --reanalyze all` — Re-analyze all sessions

1. Clear `analyzed_at` on all session records
2. Display confirmation prompt: "This will delete all {N} findings and re-analyze {M} sessions (estimated {T} hours). Continue? (y/N)". Abort if the user declines.
3. Truncate `findings` and `finding_embeddings` tables (triggers handle FTS5 cleanup)
4. Re-run Phase 2 for all sessions (same sequential processing as `--analyze`)
5. Display progress and summary as with `--analyze`

**Note:** This is a non-atomic operation. If interrupted after finding deletion but before re-analysis completes, some sessions will have no findings. This is safe — re-running `--analyze` will process all sessions with `analyzed_at IS NULL`.

### Edge cases

| Scenario | Behavior |
|----------|----------|
| Malformed JSONL file | Skip the file, log warning with session ID and reason to `processing_log`. No hard failure. Continue processing remaining files. |
| Empty session (0 messages) | Skip during Phase 1. Log as `status: 'skipped'` in `processing_log`. |
| Concurrent `/retro` runs | SQLite WAL mode handles concurrent reads. Phase 2 writes are sequential by design. No additional locking needed. |
| Database corruption | User can delete `retro.db` and re-run `--setup` then `--parse` and `--analyze` from scratch. All data is re-derivable from transcripts. |
| Disk full | Standard SQLite error handling — log error and abort current operation. |
| Very large session (&gt;10 MB JSONL) | Parse normally in Phase 1. In Phase 2, truncate context sent to Claude CLI to first 50 + last 50 messages. |
| Claude CLI invocation fails | Log as `status: 'error'` in `processing_log`, skip session, continue to next. |
| JSON parsing fails for CLI response | Log as `status: 'error'`, skip session, continue. |
| Session transcript file deleted between parse and analyze | Log as `status: 'error'` ("transcript not found"), skip, continue. |
| No sessions found in project directory | Display: "No session transcripts found. Check that Claude Code sessions exist." |
| Database does not exist when running `--parse` or `--analyze` | Display: "No retro database found. Run `/retro --setup` first." |
| `sqlite-vec` not installed | Display: "sqlite-vec extension not found. Run `bun add sqlite-vec@0.1.7-alpha.2`." |
| Embedding model not downloaded | Display: "Embedding model not found. Run `/retro --setup` to download it." |
| `sessions-index.json` missing or malformed | Continue with JSONL-only parsing. Summary field will be NULL. Log warning. |

## Constraints

- **No external API keys:** The system uses the Claude Code CLI (developer's existing auth) and local Transformers.js embeddings. Zero additional configuration required.
- **Sequential CLI processing:** One session at a time to avoid overwhelming the CLI. The initial run of 758 sessions takes an estimated 1-3 hours.
- **Local-only storage:** The SQLite database lives at `.claude/skills/retro/data/retro.db` and is gitignored. No remote sync.
- **Alpha dependency:** `sqlite-vec@0.1.7-alpha.2` is pinned to the tested version. API breakage is possible on upgrade. Acceptable for developer tooling.
- **Brute-force vector search:** sqlite-vec uses brute-force (no ANN index). Practical limit is ~100k-500k vectors. Our scale (&lt;10k findings) is well within this range.
- **First-run model download:** ~45 MB for the `all-MiniLM-L6-v2` model on first `/retro --setup`. Cached at `~/.cache/huggingface/` after first download.
- **Single project:** Only `roxabi_boilerplate` sessions are processed. Cross-project analysis is out of scope.

## Non-goals

- Cross-project analysis (only `roxabi_boilerplate` sessions)
- Real-time monitoring (no SessionEnd hooks, no cron jobs, no automation)
- Auto-applying fixes (suggest only — never auto-modify `CLAUDE.md` or standards)
- Web dashboard (CLI and skill interface only)
- Multi-user support (single developer tool)
- Custom embedding models (hardcoded to `all-MiniLM-L6-v2`)
- Tunable search weights via CLI flags (hardcoded 0.7/0.3 initially, tunable in code)

## Technical Decisions

### File Structure

```
.claude/skills/retro/
├── SKILL.md                    # /retro skill definition
├── scripts/
│   ├── parse-sessions.ts       # Phase 1
│   ├── analyze-findings.ts     # Phase 2
│   ├── recap.ts                # Phase 3
│   └── search.ts               # Semantic search
├── lib/
│   ├── db.ts                   # SQLite + sqlite-vec setup
│   ├── schema.ts               # DDL + FTS5 triggers
│   ├── parser.ts               # JSONL transcript parser
│   ├── embedder.ts             # Transformers.js wrapper
│   ├── redactor.ts             # Secret redaction
│   └── hybrid-search.ts        # BM25 + vector fusion
└── data/                       # gitignored
    └── retro.db                # SQLite database
```

### Database Schema

```sql
CREATE TABLE sessions (
  id TEXT PRIMARY KEY,
  project_path TEXT,
  git_branch TEXT,
  first_prompt TEXT,
  summary TEXT,
  message_count INTEGER,
  created_at DATETIME,
  modified_at DATETIME,
  duration_minutes REAL,
  processed_at DATETIME,
  analyzed_at DATETIME
);

CREATE TABLE findings (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  session_id TEXT REFERENCES sessions(id),
  type TEXT CHECK(type IN ('praise', 'blocker', 'suggestion', 'nitpick')),
  content TEXT NOT NULL,
  context TEXT,
  severity TEXT CHECK(severity IN ('low', 'medium', 'high')),
  tags TEXT, -- JSON array of strings, e.g. '["auth", "hooks", "performance"]'
  session_timestamp DATETIME
);

CREATE VIRTUAL TABLE findings_fts USING fts5(
  content, context, type,
  content=findings,
  content_rowid=id
);

CREATE VIRTUAL TABLE finding_embeddings USING vec0(
  finding_id INTEGER,
  embedding FLOAT[384]
);

CREATE TABLE processing_log (
  session_id TEXT NOT NULL,
  phase TEXT NOT NULL CHECK(phase IN ('parse', 'analyze')),
  processed_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
  status TEXT NOT NULL CHECK(status IN ('success', 'error', 'skipped')),
  error_message TEXT,
  PRIMARY KEY (session_id, phase)
);
```

The `content=findings` directive on the FTS5 virtual table creates a contentless shadow table that does not automatically sync. The following triggers are required to keep the FTS5 index consistent with the `findings` table:

```sql
CREATE TRIGGER findings_ai AFTER INSERT ON findings BEGIN
  INSERT INTO findings_fts(rowid, content, context, type) VALUES (new.id, new.content, new.context, new.type);
END;

CREATE TRIGGER findings_ad AFTER DELETE ON findings BEGIN
  INSERT INTO findings_fts(findings_fts, rowid, content, context, type) VALUES ('delete', old.id, old.content, old.context, old.type);
END;

CREATE TRIGGER findings_au AFTER UPDATE ON findings BEGIN
  INSERT INTO findings_fts(findings_fts, rowid, content, context, type) VALUES ('delete', old.id, old.content, old.context, old.type);
  INSERT INTO findings_fts(rowid, content, context, type) VALUES (new.id, new.content, new.context, new.type);
END;
```

### Privacy Redaction

Before storing any finding, a redaction pass replaces common secret patterns with `[REDACTED]`:

```typescript
const REDACTION_PATTERNS = [
  // API keys
  /(?:sk|pk|key|token|secret|password|api[_-]?key)[_-]?\w*[=:]\s*['"]?[\w\-\.]{16,}/gi,
  // JWTs
  /eyJ[\w-]+\.eyJ[\w-]+\.[\w-]+/g,
  // Connection strings
  /(?:postgres|mysql|mongodb|redis):\/\/[^\s'"]+/gi,
  // Base64 credentials
  /(?:Basic|Bearer)\s+[A-Za-z0-9+\/]{20,}={0,2}/g,
];
```

This is a best-effort defense appropriate for a local-only tool where the transcripts themselves are already on disk unredacted. Surrounding context is preserved so findings remain useful.

### Hybrid Search Weights

| Component | Weight | Source | Strength |
|-----------|--------|--------|----------|
| Vector search | 0.7 | `sqlite-vec` via `finding_embeddings` | Semantic similarity — finds conceptually related findings even with different wording |
| BM25 search | 0.3 | FTS5 via `findings_fts` | Exact keyword matching — precise when the user knows the right terms |

Weights are adapted from the 2ndBrain implementation as a starting point. They should be tuned empirically once actual finding data is available.

> **Fusion algorithm:** Reciprocal Rank Fusion (RRF) with k=60. Each result's score is computed as: `score = 0.7 * (1 / (k + vector_rank)) + 0.3 * (1 / (k + bm25_rank))`. RRF is preferred over raw score normalization because BM25 scores and cosine distances have different scales that are difficult to normalize reliably.

### Embedding Model

| Property | Value |
|----------|-------|
| Model | `all-MiniLM-L6-v2` |
| Dimensions | 384 |
| Runtime | `@huggingface/transformers` (Transformers.js v3) |
| Model cache | `~/.cache/huggingface/` (~45 MB) |
| API key required | No |
| Latency | Sub-second per embedding on modern hardware |

### Claude CLI Integration

- Invoke via `claude -p --output-format json` for structured output
- The prompt instructs Claude to return a JSON array of findings
- Each finding has: `type`, `content`, `context`, `severity`, `tags`
- Double JSON parse required: the outer CLI wrapper (`{"result": "...", "session_id": "..."}`) contains the inner findings JSON as a string
- If JSON parsing fails for a session, log as error and skip

### Finding Taxonomy

The four finding types are adapted from Conventional Comments (used in the project's code review standard):

| Type | Description | Developer Action |
|------|-------------|-----------------|
| `praise` | Pattern or approach that worked well | Reinforce in standards |
| `blocker` | Problem that blocked progress or caused significant friction | Fix process or tooling |
| `suggestion` | Improvement proposed by Claude or the developer | Evaluate and prioritize |
| `nitpick` | Minor style, naming, or convention issue | Track frequency, batch-fix |

### Affected Locations

| Location | Changes |
|----------|---------|
| `.claude/skills/retro/` | All new files — skill definition, scripts, library modules |
| `.gitignore` | Add `.claude/skills/retro/data/` to ignore the SQLite database |
| `CLAUDE.md` | Add `/retro` to the Available Skills table |

### Tier Assessment

**Tier: F-lite** — Clear scope, documented requirements (analysis complete), single domain (developer tooling in `.claude/skills/`). No new architecture concepts in the application itself. No cross-domain boundaries (does not touch `apps/api`, `apps/web`, or `packages/`).

## Success Criteria

- [ ] `retro --setup` downloads model, loads sqlite-vec, creates schema without errors
- [ ] `retro --parse` processes all 758 sessions; re-run skips already-processed
- [ ] `retro --parse` handles malformed JSONL gracefully (skip + log, no crash)
- [ ] `retro --analyze --limit 10` produces findings for at least 8 of 10 sessions, each finding has a non-empty content, valid type, valid severity, and at least one tag; no findings contain unredacted secrets
- [ ] `retro --analyze` full run completes with progress reporting; interrupt + resume works
- [ ] Each finding has type, content, context, severity, and tags (JSON array)
- [ ] Secret redaction removes API keys, JWTs, connection strings from stored findings
- [ ] `retro --search "authentication"` returns semantically relevant results
- [ ] `retro --search "auth" --type blocker` filters correctly
- [ ] `retro --recap` generates a trend report with blocker counts, improvements, regressions
- [ ] `retro --reanalyze <session-id>` clears and re-analyzes a single session
- [ ] Database stored at `.claude/skills/retro/data/retro.db` (gitignored)
- [ ] No external API keys required (local embeddings + Claude CLI auth)
- [ ] FTS5 triggers keep the search index in sync with findings table

## Open Questions

1. Should the `processing_log` track a separate `'embed'` phase for more granular error recovery (e.g., parse succeeds but embedding generation fails)?
2. What minimum `@huggingface/transformers` version is required for full Bun compatibility? The spec pins `^3.0.0` but the exact minimum needs validation.
3. Should the redactor also flag sessions containing secrets for user review, beyond just replacing matched patterns with `[REDACTED]`?
4. How should database schema migrations be handled if the schema evolves in future iterations? (Manual migration scripts vs. a lightweight migration framework)
5. What is the maximum session size (in messages) that can be sent to the Claude CLI without hitting token limits? The spec uses 500 messages as the truncation threshold — this needs empirical validation.
