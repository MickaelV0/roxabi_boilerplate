---
title: "Rate Limiting & API Security"
description: Analysis of rate limiting and security hardening for the NestJS/Fastify API deployed on Vercel serverless
---

## Context

Production APIs need protection against brute force attacks, abuse, and resource exhaustion. Auth endpoints (login, register, password reset) are especially sensitive targets. The Roxabi API runs on NestJS + Fastify and deploys to Vercel as serverless functions, which introduces constraints on state persistence — in-memory rate limiting resets per invocation and cannot track request rates across function instances.

Issue [#53](https://github.com/AikidoSec/roxabi_boilerplate/issues/53) captures this requirement. It depends on [#19 (Auth + Users)](https://github.com/AikidoSec/roxabi_boilerplate/issues/19) for user-based rate limiting identification.

## Questions Explored

1. **Storage backend**: What backing store works for rate limit state in a serverless environment?
2. **Rate limit tiers**: What limits should apply to different endpoint categories?
3. **Identification strategy**: How should rate-limited clients be identified (IP, user, or hybrid)?
4. **Error format**: How should 429 responses be structured?
5. **Security headers**: What additional HTTP security hardening is needed?
6. **Scope boundaries**: What belongs in this feature vs. separate concerns?

## Analysis

### Storage: Redis via Upstash

In-memory rate limiting is ineffective in serverless — each invocation starts fresh with no shared state. The solution is an external store.

**Upstash Redis** is the recommended choice:
- Designed for serverless (HTTP-based Redis, no persistent connections needed)
- Free tier covers development and low-traffic production
- Native Vercel integration (marketplace add-on)
- Compatible with `@nestjs/throttler` via custom storage adapter
- Sub-millisecond latency for rate limit checks

**Alternatives considered:**
- **Vercel KV**: Powered by Upstash under the hood but adds vendor lock-in to Vercel's pricing
- **In-memory**: Only protects against bursts within a single invocation — not viable for real rate limiting

### Rate Limit Tiers

Three tiers with **configurable limits via environment variables** and conservative defaults:

| Tier | Scope | Default | Env Var | Rationale |
|------|-------|---------|---------|-----------|
| **Global** | All endpoints | 60 req/min | `RATE_LIMIT_GLOBAL_TTL`, `RATE_LIMIT_GLOBAL_LIMIT` | Prevents general abuse |
| **Auth** | Login, register, password reset, magic link | 5 req/min | `RATE_LIMIT_AUTH_TTL`, `RATE_LIMIT_AUTH_LIMIT` | Brute force protection |
| **Public API** | Future API key-authenticated endpoints | 100 req/min | `RATE_LIMIT_API_TTL`, `RATE_LIMIT_API_LIMIT` | Per-key quota (future) |

Defaults are conservative for early-stage SaaS. Consumers adjust via environment variables without code changes.

### Client Identification: IP + User Hybrid

A hybrid approach provides the best balance:

- **Unauthenticated requests**: Identified by IP address (extracted from `x-forwarded-for` behind Vercel's proxy)
- **Authenticated requests**: Identified by user ID from the JWT/session

This prevents a single authenticated user from consuming the entire rate limit quota for a shared IP (offices, VPNs, co-working spaces) while still protecting unauthenticated endpoints by IP.

**Implementation**: A custom `ThrottlerGuard` override that inspects the request context — if an authenticated user is present, use their user ID as the tracker key; otherwise fall back to IP.

### Error Response Format

Rate limit responses follow the **existing API error format** for consistency:

```json
{
  "statusCode": 429,
  "timestamp": "2024-01-15T10:30:45.123Z",
  "path": "/auth/login",
  "correlationId": "abc-xyz-123",
  "message": "Too many requests. Please try again later.",
  "errorCode": "RATE_LIMIT_EXCEEDED"
}
```

Additionally, standard rate limit headers are included on **all** responses (not just 429s):

| Header | Description |
|--------|-------------|
| `X-RateLimit-Limit` | Maximum requests allowed in the window |
| `X-RateLimit-Remaining` | Requests remaining in the current window |
| `X-RateLimit-Reset` | Unix timestamp when the window resets |
| `Retry-After` | Seconds until the client can retry (only on 429) |

A new `RATE_LIMIT_EXCEEDED` entry is added to the `ErrorCode` registry, and a `ThrottlerExceptionFilter` maps `ThrottlerException` to the standard response format.

### Security Headers: Helmet

[`@fastify/helmet`](https://github.com/fastify/fastify-helmet) sets secure HTTP response headers:

- `Content-Security-Policy` — Prevents XSS and data injection
- `X-Content-Type-Options: nosniff` — Prevents MIME sniffing
- `X-Frame-Options: DENY` — Prevents clickjacking
- `Strict-Transport-Security` — Enforces HTTPS
- `X-XSS-Protection` — Legacy XSS filter (for older browsers)
- `Referrer-Policy` — Controls referrer information

Since this is an API (no HTML rendering), CSP can use restrictive defaults. Helmet is registered as a Fastify plugin in the bootstrap phase.

### Request Size Limits

Fastify's built-in `bodyLimit` option caps request body size. Default: **1 MB** (`1048576` bytes), configurable via `REQUEST_BODY_LIMIT` env var. This prevents resource exhaustion from oversized payloads.

For specific routes needing larger bodies (e.g., file upload endpoints in the future), per-route overrides can be applied via Fastify route options.

### Endpoint Exceptions

The following endpoints are **exempt** from rate limiting:

- `GET /health` — Health check for monitoring/load balancers
- `GET /readiness` — Readiness probe for orchestration
- Any future `/metrics` or observability endpoints

Exemption is implemented via the `@SkipThrottle()` decorator from `@nestjs/throttler`.

## Conclusions

1. **Upstash Redis** is the right storage backend for serverless rate limiting — no persistent connections, sub-millisecond latency, free tier available.
2. **Three configurable tiers** (global, auth, public API) with conservative defaults cover the requirements without over-engineering.
3. **IP + User hybrid identification** is the most robust approach, avoiding false positives on shared networks while protecting unauthenticated endpoints.
4. **Match the existing error format** for 429 responses — no need to introduce RFC 7807 at this stage. The current format already includes correlationId and errorCode.
5. **Helmet + body limits** are low-effort, high-impact security hardening that should ship alongside rate limiting.
6. **Always-on by default** — this is a security feature, not an optional module. Consumers configure limits, not whether protection exists.

## Next Steps

- Promote this analysis to a spec with implementation details (NestJS module structure, Upstash integration, decorator API)
- Create spec at `docs/specs/53-rate-limiting-api-security.mdx`
- After spec approval, scaffold and implement via `/scaffold --spec 53`
