---
title: "Bootstrap Investigation Step — Optional PoC Gate Between Analysis and Spec"
description: Analysis for adding an optional investigation/spike step to the bootstrap pipeline, allowing technical validation before committing to a spec
---

## Context

The bootstrap pipeline (`/bootstrap`, defined in `.claude/skills/bootstrap/SKILL.md`) currently follows a two-gate process: **Gate 1 (Analysis)** → **Gate 2 (Spec)**. The analysis explores the problem space and evaluates approaches, while the spec commits to a specific solution for implementation.

The gap: when the analysis reveals **technical uncertainty** — unknown API behavior, competing library choices, unclear performance characteristics — the spec is written based on assumptions rather than evidence. If those assumptions prove wrong during `/scaffold`, the team discovers the issue mid-implementation, leading to rework, spec revisions, and wasted effort.

**This analysis proposes an optional Gate 1.5 — Investigation** that allows the team to spike a solution before committing to a spec. The investigation validates technical assumptions with real code, and its findings feed directly into a more informed spec.

**Related:** [Development Process](../processes/dev-process) (the overall pipeline this modifies), [Agent Teams Architecture](./78-agent-teams-architecture) (the subagent model used for execution).

### Current flow

```
Gate 1: Analysis → Expert Review → User Approval
  ↓
Gate 2: Spec → Expert Review → User Approval
  ↓
/scaffold
```

### Proposed flow

```
Gate 1: Analysis → Expert Review → User Approval
  ↓
[Auto-detect uncertainty] → User confirms/skips
  ↓ (if confirmed)
Gate 1.5: Investigation → Findings appended to analysis
  ↓
Gate 2: Spec (informed by real data) → Expert Review → User Approval
  ↓
/scaffold
```

## Questions Explored

1. When should the investigation step be triggered — and who decides?
2. How should the investigation be scoped to prevent runaway spikes?
3. Who executes the investigation work?
4. What happens when the investigation reveals infeasibility?
5. Where do the findings live, and how do they feed into the spec?
6. How should the spike code be handled (branch lifecycle)?

## Analysis

### 1. Trigger Mechanism: Auto-Detect + User Confirms

After Gate 1 approval, the bootstrap orchestrator scans the approved analysis for **uncertainty signals**. If any are detected, it suggests investigation to the user via `AskUserQuestion`. The user can always skip.

**Uncertainty signal categories:**

| Category | Signal patterns |
|----------|----------------|
| **Explicit uncertainty markers** | "needs testing", "unclear if", "to be validated", "unknown performance", "requires investigation", trade-off sections without a concluded decision |
| **Multiple competing approaches** | Analysis lists 2+ approaches without a clear winner, or recommends one "pending validation" |
| **External dependency risk** | Analysis relies on a third-party API, library, or service that hasn't been tested in this project's context |

**Detection logic:** The orchestrator reads the approved analysis content and checks for these patterns. This is a **heuristic** — well-written analyses may use phrases like "unclear if" or "trade-off" as normal analytical language without indicating genuine technical uncertainty. The detection is intentionally liberal (it's cheap to suggest, and the user always decides). False positives are handled by the user simply choosing "Skip to spec."

**User prompt example:**

> "The analysis contains technical uncertainty (e.g., `{detected signal}`). Would you like to investigate before writing the spec?"
>
> - **Investigate** — Spike the uncertain areas before speccing
> - **Skip to spec** — Proceed directly to Gate 2

If no signals are detected, investigation is not suggested and the pipeline proceeds directly to Gate 2.

### 2. Scope Control: Question + Done Criteria

Before the investigation starts, the orchestrator **drafts** the scope and **presents it to the user for confirmation** via `AskUserQuestion`:

1. **Investigation question** — The specific technical question to answer (e.g., "Can sqlite-vec run on Bun/WSL2 with acceptable query latency?")
2. **Done criteria** — What constitutes success and failure:
   - **Success**: Conditions that validate the approach (e.g., "Query latency &lt;50ms for 10k vectors")
   - **Failure**: Conditions that invalidate the approach (e.g., "sqlite-vec segfaults on Bun, or latency >500ms")

The user reviews and may adjust the question and criteria before the spike begins. This prevents the orchestrator from answering the wrong question.

**No hard timebox** is enforced — the investigation runs until done criteria are met (success or failure). However, as a **soft safeguard**, if the subagent exceeds 15 tool-use turns without reaching done criteria, the orchestrator should collect partial findings and present them to the user, asking whether to continue or stop with what's available.

**Multiple questions:** If the analysis has multiple uncertainties, the orchestrator groups them into a single investigation with numbered questions and per-question done criteria. Single-domain questions are addressed sequentially by one subagent. Cross-domain questions (e.g., backend + frontend) are split across parallel subagents, each handling their domain's questions. The orchestrator merges findings from parallel subagents before presenting to the user.

### 3. Execution Model: Domain Expert Subagent(s)

The orchestrator spawns the appropriate domain expert(s) as subagent(s) via the `Task` tool:

| Uncertainty domain | Subagent |
|-------------------|----------|
| Architecture, feasibility, library evaluation | `architect` |
| Backend API, database, ORM | `backend-dev` |
| Frontend rendering, UI library, browser APIs | `frontend-dev` |
| CI/CD, deployment, infrastructure | `devops` |
| Cross-cutting or unclear | `architect` (default) |

**Subagent receives:**
- The approved analysis document path
- The investigation question(s) and done criteria
- Instructions to work on the spike branch worktree (`../roxabi-spike-XXX`)
- Instructions to report findings using the structured MDX template (see Section 5)

**Findings extraction:** The subagent returns findings via the `Task` tool's return value (structured text). The orchestrator formats and appends them to the analysis. If findings are too large for a single return, the subagent writes them to a file in the spike worktree and returns the file path — the orchestrator reads it before deleting the branch.

**Multiple subagents:** If the investigation spans multiple domains (e.g., backend + frontend), spawn parallel subagents with separate questions. Each reports independently. The orchestrator merges their findings into a single "Investigation Findings" section.

### 4. Infeasibility Handling: Pivot Within Investigation

If the subagent's initial approach fails (done criteria → failure), it **pivots** to test alternative approaches mentioned in the analysis before giving up:

```
Test approach A → Failure
  ↓
Test approach B (from analysis alternatives) → Success → Report findings
  ↓ (if also fails)
Test approach C → Success → Report findings
  ↓ (if ALL fail)
Report: all approaches infeasible → Return to Gate 1
```

**Rules:**
- The subagent tries alternatives **only** from approaches already mentioned in the analysis (no inventing new ones)
- Each alternative gets its own done criteria evaluation
- If **all** listed approaches fail, findings are appended to the analysis and the orchestrator **presents the situation to the user** via `AskUserQuestion`:
  > "Investigation found all approaches infeasible. Findings have been appended. How would you like to proceed?"
  > - **Revise analysis** — Return to Gate 1 with new direction
  > - **Proceed to spec anyway** — Accept the risk and write the spec with current findings
  > - **Abandon** — Stop the bootstrap pipeline
- The user is always informed of pivots (via the findings report)

### 5. Output: Findings Addendum to Analysis

Investigation findings are **appended** to the existing analysis document as a new section:

```mdx
## Investigation Findings

> Investigation conducted on {date} by {subagent_type} subagent.
> Branch: `spike/XXX-slug` (deleted after findings extraction).

### Question 1: {investigation question}

**Approach tested:** {what was tested}

**Result:** {Success | Failure | Partial}

**Findings:**
- {Key finding 1}
- {Key finding 2}
- {Performance data, compatibility notes, etc.}

**Recommendation:** {What this means for the spec}

### Question 2: {if multiple questions}
...
```

This keeps all pre-spec research in one place. The spec can reference specific findings (e.g., "Per investigation findings, approach B was selected because...").

**User review of findings:** Before appending findings to the analysis document, the orchestrator presents the findings summary to the user via `AskUserQuestion`. The user can approve (append as-is), request edits, or discard findings. This prevents incorrect or misleading findings from silently feeding into the spec.

### 6. Spike Branch Lifecycle

```bash
# 1. Create throwaway branch (slug derived from analysis, not issue number)
git worktree add ../roxabi-spike-XXX -b spike/investigation-slug staging
cd ../roxabi-spike-XXX

# 2. Subagent does investigation work (prototyping, testing, committing to spike branch)

# 3. Orchestrator extracts findings and user approves them

# 4. Guard: verify findings were appended to the analysis before cleanup
#    (orchestrator checks the analysis file was modified)

# 5. Delete branch and worktree (no code survives)
cd ../roxabi_boilerplate
git worktree remove ../roxabi-spike-XXX
git branch -D spike/investigation-slug
```

> **Note on `git branch -D`:** This is an intentional force-delete of a throwaway branch that is never meant to be merged. This is a pre-approved destructive operation within the investigation step — subagents may commit code to the spike branch during testing, but those commits are intentionally discarded.

**Branch naming:** Since the investigation runs before "Ensure GitHub Issue," the issue number may not exist yet. Spike branches use the analysis slug (not the issue number): `spike/{analysis-slug}` (e.g., `spike/sqlite-vec-feasibility`). If an issue number is available (from `--issue N` entry point), use `spike/XXX-slug` instead.

**Key rules:**
- Spike branches use the `spike/` prefix (distinct from `feat/`, `fix/`)
- Spike branches are **never merged** — they exist only for testing
- **Deletion guard:** The orchestrator verifies the analysis file was modified (findings appended) before deleting the spike branch. If extraction failed, the branch is preserved and the user is informed.
- No spike code enters the spec or scaffold — only the findings document

### Integration with Existing Pipeline

The investigation step fits into the bootstrap skill between Gate 1 approval and the "Ensure GitHub Issue" step.

**Sequencing note:** Investigation runs **before** "Ensure GitHub Issue" because not every bootstrap has an issue number at this point (bare text entry). Spike branches use the analysis slug for naming (see Section 6). The issue is created after investigation, before Gate 2.

```
Gate 1 approved
  ↓
Auto-detect uncertainty signals in analysis
  ↓
  ├── No signals → Skip to "Ensure GitHub Issue"
  └── Signals detected → AskUserQuestion
        ├── User skips → Skip to "Ensure GitHub Issue"
        └── User confirms → Run investigation
              ↓
            Draft question(s) + done criteria → User confirms
              ↓
            Create spike branch + worktree
              ↓
            Spawn domain subagent(s)
              ↓
            Collect findings (soft safeguard at 15 turns)
              ↓
            Present findings to user for review
              ↓
            Append approved findings to analysis
              ↓
            Verify findings written → Delete spike branch
              ↓
            ├── Feasible → Continue to "Ensure GitHub Issue"
            ├── All failed → User chooses: revise / proceed / abandon
            └── (revise) → Return to Gate 1
```

**No new user-approval gate is added to the pipeline.** The investigation enriches the analysis with evidence. The user reviews findings inline (not a formal gate), and the enriched analysis flows into Gate 2 (Spec) where the user approves the full spec as before.

### Issue Status Transitions

No new status is needed. The investigation happens **within** the "Analysis" status:
- Gate 1 approved → status = "Analysis" (already set)
- Investigation runs (status stays "Analysis")
- Gate 2 approved → status = "Specs" (existing transition)

## Conclusions

1. **The investigation step fills a real gap** — It de-risks specs by validating assumptions with real code before committing to a solution.
2. **Auto-detect + user confirms** is the right trigger model — It surfaces uncertainty without adding friction to clear-scope features.
3. **Scoped question + done criteria** prevents runaway spikes without needing arbitrary timeboxes.
4. **Domain subagent execution** leverages the existing agent architecture — no new agent types needed.
5. **Findings as an analysis addendum** keeps all pre-spec research in one document, making specs better-informed.
6. **Throwaway spike branches** ensure no prototype code leaks into production — only findings survive.
7. **Pivoting within investigation** handles infeasibility gracefully, only escalating to Gate 1 revision when all options are exhausted.

## Impacted Files

The spec will need to define changes to:

| File | Change |
|------|--------|
| `.claude/skills/bootstrap/SKILL.md` | Add investigation step between Gate 1 and "Ensure GitHub Issue" |
| `docs/processes/dev-process.mdx` | Add `spike/` to naming conventions, document investigation in the Phase 1 flow |
| `CLAUDE.md` | Add `spike/` prefix to branch conventions, update bootstrap flow description |

## Next Steps

- Create a spec defining the exact implementation changes to the files listed above
- Define the investigation prompt template for subagents (the exact prompt the orchestrator sends to the domain expert)
- Add `spike/` to the branch naming conventions in dev-process.mdx
