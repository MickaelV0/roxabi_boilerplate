---
title: "Brainstorm: windagency/valora.ai Analysis"
description: Analysis of windagency/valora.ai for epic #163
type: brainstorm
---

## Context

**GitHub sub-issue:** [#165](https://github.com/roxabi/boilerplate/issues/165)
**Repository:** [windagency/valora.ai](https://github.com/windagency/valora.ai)
**Stars / Activity:** 5 stars, last commit 2026-02-16

## Summary Table

| Axis | Rating | One-liner |
|------|--------|-----------|
| What it does | ðŸŸ¢ | Full SDLC orchestration via AI agents â€” a superset of what roxabi automates |
| How it works | ðŸŸ¢ | 8-phase pipeline with 11 specialised agents, session persistence, and three execution tiers |
| Architecture | ðŸŸ¢ | Clean Architecture layering with enforced arch-unit-ts tests â€” rare and valuable |
| File structure | ðŸŸ¡ | `.ai/` as a self-contained engine directory is clever but different trade-off |
| Tech stack | ðŸŸ¡ | Node+pnpm vs Bun; similar TypeScript 5.x but adds multi-LLM, Ink CLI, Testcontainers |
| DX | ðŸŸ¢ | Devcontainer-first, Volta pinning, lint-staged, strong CLI ergonomics |
| Testing | ðŸŸ¢ | 8 test suites including architecture, security, performance, and acceptance tests |
| CI/CD | ðŸ”´ | No public CI/CD pipeline visible; entirely local/devcontainer-based |
| Documentation | ðŸŸ¢ | Comprehensive layered docs (user guide, developer guide, architecture) with ADR-quality agent specs |
| Unique ideas | ðŸŸ¢ | arch-unit-ts enforcement, MCP security registry, session-based state, dynamic agent selection |

## Detailed Analysis

### 1. What It Does ðŸŸ¢

VALORA (Versatile Agent Logic for Orchestrated Response Architecture) is an AI orchestration engine for the complete software development lifecycle. Rather than a SaaS product boilerplate, it is a **meta-tool** that coordinates AI agents to automate how software is built.

- **Purpose:** Replace the ad-hoc, prompt-by-prompt use of AI with a structured, governed, repeatable workflow from requirements capture through PR creation.
- **Target audience:** Developers and teams that use AI-assisted coding tools (Claude Code, Cursor, Copilot) and want to impose process, traceability, and quality gates on top of raw AI output.
- **Problem solved:** AI tools are stateless, context bleeds between sessions, planning/implementation/review are disconnected, governance is absent. VALORA binds all these into a single CLI with persistent session state.

Relative to roxabi_boilerplate, VALORA is upstream: roxabi is the SaaS product a team builds, VALORA is the operating model for the AI team that builds it. The two are complementary rather than competing.

### 2. How It Works ðŸŸ¢

VALORA exposes a `valora` CLI (built with Commander + Ink/React) that maps to an **8-phase development lifecycle**:

```
Initialisation â†’ Task Prep â†’ Planning â†’ Implementation
              â†’ Validation â†’ Review â†’ Commit & PR â†’ Feedback
```

Each lifecycle stage is backed by:
1. **Commands** â€” structured Markdown specs in `.ai/commands/` that declare which agent handles the task, which tools are allowed, which prompt pipeline stages to execute, and which LLM model to use.
2. **Agents** â€” YAML-frontmatter Markdown files in `.ai/agents/` that define a persona, expertise, autonomy level, and constraints (e.g., frontend engineer cannot touch backend code).
3. **Prompts** â€” numbered folder hierarchy `01_onboard/`, `02_context/`, `03_plan/`, etc., mapping directly to lifecycle phases. Each prompt is a standalone Markdown file consumed by the engine.
4. **Session** â€” all context (task description, decisions, plan, implementation steps) is persisted to `.ai/sessions/` so commands can pick up where a prior command stopped.
5. **Three execution tiers:** MCP Sampling (Cursor-native, free), Guided Completion (semi-automated), API Fallback (fully autonomous via LLM API keys).

The CLI selects the appropriate specialist agent dynamically based on which files are affected and what the task description says. Model assignment is strategic: GPT-5 Thinking for deep planning, Claude Sonnet for implementation/review, Claude Haiku for fast mechanical tasks.

### 3. Architecture ðŸŸ¢

VALORA's engine (`.ai/.bin/src/`) follows a strict **Clean Architecture** layering enforced by `arch-unit-ts` automated tests:

```
CLI Layer â†’ Orchestrator â†’ Agent Layer â†’ LLM Layer
         â†’ Session       â†’ Config      â†’ MCP
         â†’ Services      â†’ Output      â†’ Types
```

Key design decisions:
- **Dependency Inversion via DI container** (`src/di/`) â€” providers are registered and injected, making LLM back-ends and agent registries swappable.
- **Circular dependency detection** â€” dedicated architecture test (`tests/architecture/circular-dependencies.test.ts`) prevents dependency cycles.
- **Module boundary enforcement** â€” `tests/architecture/module-boundaries.test.ts` validates that the frontend engineer agent cannot import from backend namespaces, etc. This mirrors the repo's own concerns about domain separation.
- **Plugin surface** â€” commands, agents, and LLM providers all have registry JSON files plus template files for extension without touching core engine code.

Relative to roxabi_boilerplate, which delegates architecture governance to developer discipline and code review, VALORA **codifies and automatically tests** architectural rules. This is a significant capability gap.

### 4. File / Project Structure ðŸŸ¡

```
valora.ai/
â”œâ”€â”€ .ai/                    # The entire AI engine (agents, commands, prompts, docs, sessions)
â”‚   â”œâ”€â”€ .bin/               # Compiled TypeScript engine (package.json, src/, tests/)
â”‚   â”œâ”€â”€ agents/             # 11 agent definitions + registry.json
â”‚   â”œâ”€â”€ commands/           # 16+ command specs + registry.json
â”‚   â”œâ”€â”€ prompts/            # Numbered phase folders with atomic prompt files
â”‚   â”œâ”€â”€ documentation/      # User guide, developer guide, architecture docs
â”‚   â”œâ”€â”€ sessions/           # Runtime session state (gitignored content)
â”‚   â”œâ”€â”€ templates/          # Document output templates
â”‚   â”œâ”€â”€ external-mcp.json   # MCP server registry with security metadata
â”‚   â””â”€â”€ config.json         # Engine runtime configuration
â”œâ”€â”€ .cursor/                # Cursor IDE MCP integration
â”œâ”€â”€ .devcontainer/          # Devcontainer definition
â”œâ”€â”€ knowledge-base/         # Project-level knowledge docs (CODE-QUALITY-GUIDELINES, LANGUAGE_CONVENTION)
â”œâ”€â”€ examples/               # Example usage files
â”œâ”€â”€ AGENTS.md               # AI team operating manual
â””â”€â”€ README.md
```

The key structural decision is keeping the entire AI operating system inside `.ai/` â€” this makes the tool portable: drop `.ai/` into any project and the full workflow becomes available. The engine itself is a nested Node.js package inside `.ai/.bin/`.

Compared to roxabi_boilerplate's clear `apps/` + `packages/` monorepo split, VALORA's structure is intentionally flat and tool-centric rather than product-centric. The `.ai/` nesting is a good pattern for keeping the meta-tooling out of the application source, but it creates a non-standard packaging story (the `valora` binary must be installed globally or linked from `.ai/.bin`).

### 5. Tech Stack ðŸŸ¡

| Aspect | valora.ai | roxabi_boilerplate |
|---|---|---|
| Runtime | Node.js 22.21.0 (Volta-pinned) | Bun 1.3.9 |
| Language | TypeScript 5.x (strict) | TypeScript 5.x (strict) |
| Package manager | pnpm 10.19.0 (Corepack) | Bun (built-in) |
| Linter | ESLint 9.x + eslint-plugin-perfectionist | Biome |
| Formatter | Prettier 3.x | Biome |
| Build | tsc + tsc-alias | Bun native / Vite |
| Test runner | Vitest 1.x | Vitest 4.x |
| CLI framework | Commander + Ink (React) + Chalk | N/A |
| LLM SDKs | @anthropic-ai/sdk, openai, @google/generative-ai | N/A |
| MCP | @modelcontextprotocol/sdk | N/A |
| Validation | Zod | Zod |
| Git hooks | Husky + lint-staged | Lefthook |
| Arch testing | arch-unit-ts | None |
| Containerisation | Docker devcontainer | Docker Compose (DB) |

The choice of Prettier + ESLint vs Biome is a common divergence. VALORA's ESLint config is notably exhaustive â€” `eslint-plugin-perfectionist` for import/member sorting, `eslint-plugin-sort` for alphabetical ordering, `eslint-plugin-unused-imports` â€” enforcing a very high style consistency bar.

Volta over `.nvmrc` over Bun for runtime pinning is another valid approach. Volta is transparent to the developer and enforces exact versions without explicit `nvm use` calls.

### 6. Developer Experience (DX) ðŸŸ¢

Strong points:
- **Devcontainer first** â€” the `.devcontainer/devcontainer.json` defines a full, reproducible development environment based on `mcr.microsoft.com/devcontainers/javascript-node:24`. Docker socket is mounted for Testcontainers. Persistent bash history. VS Code extensions pre-configured.
- **Volta for toolchain pinning** â€” `package.json` declares exact `node` and `pnpm` versions under `"volta"`, making the correct toolchain silently active without any developer action.
- **lint-staged** â€” only runs linting on staged files, keeping pre-commit fast regardless of project size.
- **preinstall guard** â€” `"preinstall": "pnpx only-allow pnpm"` prevents accidentally using npm or yarn, eliminating lockfile drift.
- **Rich CLI output** â€” Ink (React-based terminal UI) enables structured, styled output rather than raw console logs.
- **Config wizard** â€” `valora config setup` guides users through initial API key and provider configuration.

Weaknesses:
- No `.env.example` pattern visible â€” API keys for OpenAI/Anthropic/Google must be configured manually.
- Global install requirement (`pnpm link`) adds friction for first-time setup compared to a single `bun install`.

roxabi_boilerplate has stronger env management (`.env.example`, `check-env-sync.ts` script) but lacks the devcontainer and Volta ergonomics.

### 7. Testing Strategy ðŸŸ¢

VALORA has one of the most comprehensive testing strategies observed in a TypeScript project of this size:

| Suite | Location | Notes |
|---|---|---|
| Unit | `src/**/*.test.ts` | Per-module co-location |
| Integration | `tests/integration/` | Uses Testcontainers (PostgreSQL, Redis, LocalStack) â€” no mocks |
| E2E | `tests/e2e/` | Full CLI command invocation |
| Acceptance | `tests/acceptance/` | BDD-style user workflow scenarios |
| Security | `tests/security/` | Automated security validation |
| Performance | `tests/performance/` | Agent selection and pipeline benchmarks |
| Error scenarios | `tests/error-scenarios/` | Dedicated error-path coverage |
| Architecture | `tests/architecture/` | arch-unit-ts â€” enforces module boundaries, dependency rules, no circular deps |

Coverage thresholds are enforced at 70% for branches, functions, lines, and statements. Test timeout is 30 seconds globally with 2-minute override for architecture tests. CI pool uses threads; local uses forks for isolation.

The `AGENTS.md` mandates Testcontainers for all integration tests â€” mock-based integration tests are **explicitly forbidden**. This is a high-quality constraint that roxabi_boilerplate does not yet enforce.

### 8. CI/CD Pipelines ðŸ”´

No `.github/workflows/` directory was found in the repository. The devcontainer setup suggests the expectation is that all checks run locally (pre-commit via Husky) rather than in a remote CI pipeline.

This is a significant gap relative to roxabi_boilerplate, which has:
- `ci.yml` â€” lint, typecheck, test on every push
- `deploy-preview.yml` â€” automated Vercel preview deploys on staging pushes
- `neon-cleanup.yml` â€” database branch lifecycle management

VALORA's local-first model has the advantage of keeping all secrets on the developer's machine (LLM API keys never leave the dev environment), but there is no remote safety net, no PR-required checks, and no automated deployment pipeline.

### 9. Documentation Quality ðŸŸ¢

Documentation is a first-class concern in VALORA:

- **README.md** â€” 21 KB, fully structured with architecture diagrams, command reference tables, use case workflows, technology matrix, and innovation highlights. One of the most complete READMEs in this analysis series.
- **AGENTS.md** â€” 7.7 KB operational directory defining agent personas, autonomy levels, escalation matrix, and mandatory tooling. This doubles as both human onboarding and AI agent instruction.
- **`.ai/documentation/`** â€” separated into user guide, developer guide, and architecture sub-trees.
- **`knowledge-base/`** â€” project-level `CODE-QUALITY-GUIDELINES.md` and `LANGUAGE_CONVENTION.md` provide explicit written standards that agents can reference during code review and implementation.
- **Agent definitions** â€” each `.ai/agents/*.md` file uses YAML frontmatter for structured metadata (role, version, expertise list, tone, autonomy) and prose for behavioral instructions. Schema validation via `.ai/agents/_meta/schema.json`.
- **Command definitions** â€” `.ai/commands/*.md` declare allowed tools, dynamic agent selection criteria, execution model, and prompt pipeline stages. Self-documenting and machine-readable.

roxabi_boilerplate has excellent process documentation (dev-process.mdx, standards/) but it is primarily aimed at the human developer. VALORA's docs serve double duty â€” they are also the AI agent's operating instructions, making them more granular and structured.

### 10. Unique / Novel Ideas ðŸŸ¢

Several ideas in VALORA stand out as worth adopting or adapting:

**1. Architecture-as-Tests via arch-unit-ts**
Using `arch-unit-ts` to write tests that enforce architectural rules (no circular deps, module boundary violations, layer dependency direction) is rare and extremely valuable. It converts architectural decisions from docs into runnable assertions.
- `tests/architecture/module-boundaries.test.ts`
- `tests/architecture/circular-dependencies.test.ts`
- `tests/architecture/dependency-rules.test.ts`

**2. Structured MCP Security Registry (`external-mcp.json`)**
A JSON registry that tracks every external MCP server with: risk level (Low/Medium/High/Critical), capabilities list, `requires_approval` flag, `remember_approval: "session"` behaviour, and audit logging toggle. This turns MCP usage from an implicit trust decision into an explicit, documented, and auditable one.

**3. Session-Based Context Persistence**
`.ai/sessions/` persists the full decision trail â€” requirements, clarifications, plan, implementation steps â€” across CLI commands. A `valora plan` writes a session; `valora implement` reads it. This solves the statelessness problem of raw AI chat.

**4. Dual-Language Convention**
`LANGUAGE_CONVENTION.md` explicitly separates code language (American English: `color`, `optimize`) from documentation language (British English: `colour`, `optimise`). Unusual but thoughtful for international teams.

**5. Confidence Escalation Thresholds**
The escalation matrix in `AGENTS.md` mandates that agents halt and ask for human input if confidence drops below 70% (80% for SecOps). This is a concrete, measurable human-in-the-loop mechanism.

**6. Dynamic Agent Selection**
The `implement` command analyzes affected file paths and task description at runtime to select the most appropriate specialist agent â€” rather than always using the same generic agent.

**7. Testcontainers Mandate**
Making Testcontainers mandatory (and mock-based integration tests forbidden) for all integration tests is an opinionated and quality-raising constraint that prevents the "works with mocks, broken in production" trap.

### 11. What They Do Better Than roxabi_boilerplate

**Architecture enforcement (arch-unit-ts):**
VALORA has automated tests that verify architectural constraints â€” no circular dependencies, correct layer direction, module boundary respect. roxabi_boilerplate has no equivalent; architecture is enforced through code review and documentation only.
- Relevant files: `.ai/.bin/tests/architecture/`, `.ai/.bin/arch-unit-ts.json`

**Testing depth and diversity:**
8 distinct test suites including architecture, security, performance, and acceptance. roxabi_boilerplate has unit + e2e but no architecture tests, no security test suite, and no performance benchmarks.
- Relevant files: `.ai/.bin/tests/` (all subdirectories)

**Devcontainer + Volta toolchain reproducibility:**
The combination of a fully defined devcontainer and Volta-pinned toolchain ensures every developer â€” regardless of local machine â€” has an identical environment. roxabi_boilerplate uses Docker Compose for the database but not for the full developer environment.
- Relevant files: `.devcontainer/devcontainer.json`, `.ai/.bin/package.json` (volta field)

**Structured MCP governance:**
The `external-mcp.json` registry with risk classification, approval requirements, and audit logging is a production-grade approach to MCP tool management. roxabi_boilerplate's `.cursor/mcp.json` lists servers without security metadata.
- Relevant file: `.ai/external-mcp.json`

**Lint-staged (pre-commit speed):**
VALORA uses `lint-staged` to run ESLint/Prettier only on staged files, keeping pre-commit hooks fast at scale. roxabi_boilerplate's Lefthook biome hook globs all staged files which is equivalent in effect but lint-staged is more ecosystem-universal.

**Testcontainers mandate:**
Forbidding mock-based integration tests and requiring Testcontainers ensures integration tests use real external dependencies. This catches integration bugs that mocked tests miss.

**Agent persona structure:**
Agent files in `.ai/agents/` have rich YAML frontmatter (role, version, experimental flag, tone, expertise list) and a JSON schema (`_meta/schema.json`) for validation. roxabi's agent definitions in `.claude/agents/` are simpler Markdown without structured metadata or schema validation.

### 12. What They Do Better Than 2ndBrain

2ndBrain is a Python-based personal productivity system (Google Workspace + Telegram bot + vector search knowledge base). The comparison is asymmetric â€” different problem domains â€” but some observations apply:

**Typed, compiled language throughout:**
VALORA uses TypeScript 5.x with strict mode everywhere. 2ndBrain is Python 3.10+ with no static typing enforcement. For a complex agentic orchestration system, TypeScript's type safety catches more bugs at compile time.

**Formal test organization:**
VALORA's 8-suite test taxonomy (unit / integration / e2e / acceptance / security / performance / error-scenarios / architecture) vs 2ndBrain's flatter `tests/unit/` + `tests/integration/` structure. VALORA's explicit security and performance suites are missing from 2ndBrain.

**Architecture testing:**
2ndBrain has no equivalent of arch-unit-ts. Its architectural rules (cli/ only imports from services/, etc.) are documented but not automatically verified.

**MCP-native design:**
VALORA is built from the ground up around MCP protocols. 2ndBrain predates widespread MCP adoption and uses direct API calls. For AI tool integration, VALORA's MCP-first approach is more future-proof.

**CLI ergonomics:**
VALORA's CLI uses Ink (React-based terminal UI) for rich, interactive output â€” progress spinners, structured tables, colour-coded phases. 2ndBrain's CLI is functional but raw Python argparse/click.

The areas where 2ndBrain leads: Python has richer ML/embedding libraries (sentence-transformers, sqlite-vec) that VALORA does not need for its use case, and 2ndBrain's Telegram bot integration is mature and production-deployed.

### 13. Key Takeaways

**Priority: Must-Have**

1. **Adopt arch-unit-ts for architecture enforcement** â€” Add an architecture test suite to roxabi_boilerplate using `arch-unit-ts` to codify and automatically verify layer direction, module boundary rules, and absence of circular dependencies. This upgrades architectural governance from "documented and hoped for" to "enforced and tested."
   - Potential issue: "feat: add architecture test suite with arch-unit-ts"

2. **Add architecture and security test suites** â€” Alongside existing unit/e2e tests, introduce dedicated `tests/architecture/` and `tests/security/` suites. The architecture suite verifies structural constraints; the security suite validates input sanitisation, secret handling, and auth boundary enforcement.
   - Potential issue: "feat: expand test strategy with architecture and security suites"

3. **Structured MCP security registry** â€” Adopt VALORA's `external-mcp.json` pattern for documenting MCP servers with risk classification, required approvals, and audit logging flags. Adapt this into roxabi's `.cursor/mcp.json` or create a new governance file.
   - Potential issue: "chore: add MCP security registry with risk classification"

**Priority: Nice-To-Have**

4. **Devcontainer for full environment reproducibility** â€” Add a `.devcontainer/devcontainer.json` to roxabi_boilerplate to standardise the developer environment beyond the database Docker Compose. Include VS Code extension recommendations and docker socket mounting for Testcontainers support.
   - Potential issue: "chore: add devcontainer configuration"

5. **Confidence escalation thresholds in agent definitions** â€” Add explicit `confidence_threshold` and `escalation_criteria` fields to roxabi's `.claude/agents/*.md` files, mirroring VALORA's AGENTS.md escalation matrix. This makes human-in-the-loop behaviour explicit and reproducible.
   - Potential issue: "docs: add escalation thresholds and confidence criteria to agent definitions"

## What's next?

- Create GitHub issue for adopting `arch-unit-ts` (highest ROI â€” converts docs into running assertions)
- Create GitHub issue for security test suite (currently absent from roxabi)
- Evaluate VALORA's session-persistence model as a complement to roxabi's existing process: could `.ai/sessions/` pattern inspire a feature-spec progress tracker that persists Claude's context across `/scaffold` runs?
- Review `.ai/agents/_meta/schema.json` and apply equivalent JSON schema validation to `.claude/agents/*.md` frontmatter
- Consider adopting VALORA's `preinstall: only-allow` guard (equivalent for Bun) to prevent accidental npm/pnpm installs in the monorepo
